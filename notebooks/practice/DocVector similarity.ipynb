{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc46262c-ff8f-4eee-a689-d834910698ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 91 kB/s eta 0:00:011     |██████████████▏                 | 10.6 MB 2.4 MB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.21.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 434 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695ff3f4-7ba2-4183-9e3d-71664a5e8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ce487e-d259-456b-bbd3-28c3891c6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서의 수 : 2382\n"
     ]
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/data.csv\", filename=\"data.csv\")\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print('전체 문서의 수 :',len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b9818-3ed5-4bff-a945-9d495cd0e213",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c5bc14-dc3a-4846-9450-7f2f77e6e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "df['cleaned'] = df['Desc'].apply(_removeNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d6d909-47ef-4734-9441-a35ba9a7e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    we know that power is shifting: from west to e...\n",
       "1    following the success of the accidental billio...\n",
       "2    how to tap the power of social software and ne...\n",
       "3    william j. bernstein is an american financial ...\n",
       "4    amazing book. and i joined steve jobs and many...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "df['cleaned'] = df.cleaned.apply(make_lower_case)\n",
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01191aac-e1c0-4c2a-8f81-33e18839d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02dd5a07-3e05-40cf-ab27-d0763a7726fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    know power shifting: west east north south, pr...\n",
       "1    following success accidental billionaires mone...\n",
       "2    tap power social software networks build busin...\n",
       "3    william j. bernstein american financial theori...\n",
       "4    amazing book. joined steve jobs many akio mori...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "df['cleaned'] = df.cleaned.apply(remove_stop_words)\n",
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91a7b7d-78fd-42cb-8ac1-37a599b530c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    know power shifting west east north south pres...\n",
       "1    following success accidental billionaires mone...\n",
       "2    tap power social software networks build busin...\n",
       "3    william j bernstein american financial theoris...\n",
       "4    amazing book joined steve jobs many akio morit...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "df['cleaned'] = df.cleaned.apply(remove_punctuation)\n",
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152b5888-9f75-455e-8560-490e7f063b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "df['cleaned'] = df.cleaned.apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777e4a89-5f54-4e2e-85ad-d47acc6ee313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    know power shifting west east north south pres...\n",
       "1    following success accidental billionaires mone...\n",
       "2    tap power social software networks build busin...\n",
       "3    william j bernstein american financial theoris...\n",
       "4    amazing book joined steve jobs many akio morit...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6142e2-0454-4c19-9751-35798d47f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서의 수 : 2381\n"
     ]
    }
   ],
   "source": [
    "df['cleaned'].replace('', np.nan, inplace=True)\n",
    "df = df[df['cleaned'].notna()]\n",
    "print('전체 문서의 수 :',len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9bd96-ef9f-43a9-a391-f5e18b35c0d1",
   "metadata": {},
   "source": [
    "토큰화 --> W2V 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7077818a-9f44-4e2a-9703-48a610a0b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for words in df['cleaned']:\n",
    "    corpus.append(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21a4143b-a7ed-44ba-81d4-433111578c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381 170 160\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), len(corpus[0]), len(corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33134a6-4866-480b-8b55-1cafd43d32c8",
   "metadata": {},
   "source": [
    "### 사전훈련된 워드 임베딩 사용\n",
    "사전 훈련된 워드 임베딩을 단어 벡터 초기값으로 사용하여 성능 높힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d96640d-265d-443a-9507-4aed4d6778df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3373612, 3981030)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "#                            filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=corpus)\n",
    "word2vec_model.build_vocab(corpus)\n",
    "# word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)\n",
    "word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df84f4-0a9b-4b65-9390-d3be3e029882",
   "metadata": {},
   "source": [
    "### 단어 벡터 평균 구하기\n",
    "각 문서에 존재하는 단어들의 벡터값 평균을 문서의 벡터값으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bf7dccc-eed6-4adb-84a9-37d730048992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vectors(document_list):\n",
    "    document_embedding_list = []\n",
    "\n",
    "    # 각 문서에 대해서\n",
    "    for line in document_list:\n",
    "        doc2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in word2vec_model.wv:\n",
    "                count += 1\n",
    "                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
    "                if doc2vec is None:\n",
    "                    doc2vec = word2vec_model.wv.key_to_index[word]\n",
    "                else:\n",
    "                    doc2vec = doc2vec + word2vec_model.wv.key_to_index[word]\n",
    "\n",
    "        if doc2vec is not None:\n",
    "            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n",
    "            doc2vec = doc2vec / count\n",
    "            document_embedding_list.append(doc2vec)\n",
    "\n",
    "    # 각 문서에 대한 문서 벡터 리스트를 리턴\n",
    "    return document_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e1776f9-b7cd-42fc-b5c8-7d8962377a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 벡터의 수 : 2381\n"
     ]
    }
   ],
   "source": [
    "document_embedding_list = get_document_vectors(df['cleaned'])\n",
    "print('문서 벡터의 수 :',len(document_embedding_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b31e4-a8a8-43c5-a525-266b1114cc11",
   "metadata": {},
   "source": [
    "### 추천 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eefdbb3-4a39-421c-9e31-80893f6716a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1579.30612245 1652.19285714  754.02564103 ... 1625.42708333 1567.64893617\n 1830.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1289/1387879087.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosine_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_embedding_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_embedding_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'코사인 유사도 매트릭스의 크기 :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    144\u001b[0m                             estimator=estimator)\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0m\u001b[1;32m    147\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         estimator=estimator)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    695\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1579.30612245 1652.19285714  754.02564103 ... 1625.42708333 1567.64893617\n 1830.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)\n",
    "print('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b41eb8-d293-4f94-b1be-b58c6355e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title):\n",
    "    books = df[['title', 'image_link']]\n",
    "\n",
    "    # 책의 제목을 입력하면 해당 제목의 인덱스를 리턴받아 idx에 저장.\n",
    "    indices = pd.Series(df.index, index = df['title']).drop_duplicates()    \n",
    "    idx = indices[title]\n",
    "\n",
    "    # 입력된 책과 줄거리(document embedding)가 유사한 책 5개 선정.\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "\n",
    "    # 가장 유사한 책 5권의 인덱스\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 전체 데이터프레임에서 해당 인덱스의 행만 추출. 5개의 행을 가진다.\n",
    "    recommend = books.iloc[book_indices].reset_index(drop=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "\n",
    "    # 데이터프레임으로부터 순차적으로 이미지를 출력\n",
    "    for index, row in recommend.iterrows():\n",
    "        response = requests.get(row['image_link'])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        fig.add_subplot(1, 5, index + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe93105-1442-40c5-b921-fae1d5ef933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations(\"The Da Vinci Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f0427-d64b-469f-b932-5057c237921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations(\"The Murder of Roger Ackroyd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf79583-47c4-4614-9788-0621db7958cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
