{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007f6fd7-0ff1-4a0f-b3fe-097a6ab62354",
   "metadata": {},
   "source": [
    "# UDF\n",
    "User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ccaeb52-d634-4fb8-ade8-026e0422dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30046ef5-66c9-49f4-9754-803b404f71d2",
   "metadata": {},
   "source": [
    "create a python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2aba91-8cf6-42be-b236-82444c894b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce007b-ead7-4dac-8301-4747b67ab4f6",
   "metadata": {},
   "source": [
    "Convert a python function to pyspark UDF\n",
    "- passing the function to PySpark SQL udf()\n",
    "- udf() function returns pyspark.sql.functions UserDefinedFunction class object\n",
    "\n",
    "Using UDF with select()\n",
    "- use convertUDF() on a DataFrame column as a regular build-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebcfd1ac-dbb5-4fdc-b413-7a4e8f4c9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Converting function to UDF \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z))\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326fe29-bcdf-4155-ac4a-fad8f83e37ed",
   "metadata": {},
   "source": [
    "# Other UDF explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf1e9f9-4cae-494e-ad68-50cfe435316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "\n",
    "conf = pyspark.SparkConf() \n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"sales\", FloatType(),True),    \n",
    "StructField(\"employee\", StringType(),True),\n",
    "StructField(\"ID\", IntegerType(),True)\n",
    "])\n",
    "data = [[ 10.2, \"Fred\",123], [18.5, \"Yeong\", 234]]\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffb2ab0-9980-462d-9608-d1ab9b57b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+\n",
      "|sales|employee| ID|\n",
      "+-----+--------+---+\n",
      "| 10.2|    Fred|123|\n",
      "| 18.5|   Yeong|234|\n",
      "+-----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396e201-6205-4fe3-97b9-bc506b291d84",
   "metadata": {},
   "source": [
    "### 1. withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f00cf17-2c56-4056-a254-9316e5441f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 06:18:38 WARN SimpleFunctionRegistry: The function colsint replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# colsInt 라는 함수를 생성하고 \n",
    "colsInt = udf(lambda z : toInt(z), IntegerType())\n",
    "\n",
    "# 등록한다.\n",
    "# 1st 인수 : 이 함수를 우리가 refer할 때 사용할 이름\n",
    "# 2nd 인수 : 등록할 함수\n",
    "spark.udf.register(\"colsInt\", colsInt)\n",
    "\n",
    "# 등록된 함수는 toInt() 라는 다른 함수를 호출한다. - 얘는 등록할 필요 없다.\n",
    "def toInt(s):\n",
    "    if isinstance(s, str) == True: # isinstance(얘가, 이타입이면true)\n",
    "        st = [str(ord(i)) for i in s]  # ord() : Unicode character가 나타내는 integer 리턴\n",
    "        return (int(''.join(st)))\n",
    "    else:\n",
    "        return Null\n",
    "    \n",
    "# 이제 colsInt 함수를 호출한다.\n",
    "# 1st 인수 : 우리가 생성하고자 하는 새로운 컬럼 이름\n",
    "# 2nd 인수 : 함수에 plug in 하고자 하는 데이터프레임 컬럼\n",
    "# df['employee'] 는 column object라는 걸 기억하자. (single employee가 아니다!) \n",
    "df2 = df.withColumn('semployee', colsInt('employee'))\n",
    "\n",
    "# 따라서 우리는 해당 컬럼의 모든 row 를 looping해야한다. --> colsInt = udf(lambda z : toInt(z), Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fbf1d4-f9cb-49d6-9e5c-95ed7e1dcf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+----------+\n",
      "|sales|employee| ID| semployee|\n",
      "+-----+--------+---+----------+\n",
      "| 10.2|    Fred|123|1394624364|\n",
      "| 18.5|   Yeong|234|2014554583|\n",
      "+-----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d116ce-5543-4046-bb86-e1672252a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 06:34:30 WARN SimpleFunctionRegistry: The function colsstr replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+---+\n",
      "|sales|employee| ID|sid|\n",
      "+-----+--------+---+---+\n",
      "| 10.2|    Fred|123|123|\n",
      "| 18.5|   Yeong|234|234|\n",
      "+-----+--------+---+---+\n",
      "\n",
      "root\n",
      " |-- sales: float (nullable = true)\n",
      " |-- employee: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- sid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이건 역으로  int -> string 으로 바꾸는 연습\n",
    "colsStr = udf(lambda x : toString(x), StringType())\n",
    "spark.udf.register(\"colsStr\", colsStr)\n",
    "def toString(i):\n",
    "    if isinstance(i, int) == True:\n",
    "        return str(i)\n",
    "    else:\n",
    "        return Null\n",
    "df3 = df.withColumn('sid', colsStr('ID'))\n",
    "df3.show()\n",
    "df3.printSchema() # sid type string인거 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd63f75-de83-43de-a0a6-adde8d3e6a74",
   "metadata": {},
   "source": [
    "### 2. Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "772c4e52-0fb6-45b1-851d-bd9b07c2c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+----------+\n",
      "|sales|employee| ID| iemployee|\n",
      "+-----+--------+---+----------+\n",
      "| 10.2|    Fred|123|1394624364|\n",
      "| 18.5|   Yeong|234|2014554583|\n",
      "+-----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql 구문을 먹이기 위해서 dataframe을 table로 등록한다.\n",
    "# df : 데이터프레임 , dftab : 임시 테이블\n",
    "spark.registerDataFrameAsTable(df, \"dftab\")\n",
    "\n",
    "# employee 컬럼에 colsInt 함수 먹이기\n",
    "df4 = spark.sql(\"select sales, employee, ID, colsInt(employee) as iemployee from dftab\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e9696-2b26-4140-821b-1c43da8aa544",
   "metadata": {},
   "source": [
    "### 3. RDD Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c3f6317-d4d4-4048-ba87-ba3fcf9169d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"name\"으로 employee element를 연속해서 참조한 후,\n",
    "# 해당 필드의 각 letter를 integer로 바꾸고 concatenate\n",
    "def toIntEmployee(rdd):\n",
    "    s = rdd[\"employee\"]\n",
    "    if isinstance(s, str):\n",
    "        st = [str(ord(i)) for i in s]\n",
    "        e = int(''.join(st))\n",
    "    else:\n",
    "        e = s\n",
    "    return Row(rdd[\"sales\"], rdd[\"employee\"], rdd[\"ID\"], e)\n",
    "\n",
    "# DataFrame은 map() 함수가 없기 때문에, map()을 쓰려면 dff.rdd 를 써서 datafrmae을 RDD로 바꿔줘야한다.\n",
    "# 얘는 toIntEmployee 함수에 row object를 리턴한다.\n",
    "# RDD 는 immutable 하기 때문에, 새로운 row를 만들어줘야한다.\n",
    "rdd = df.rdd.map(toIntEmployee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d269de1-8148-492d-89df-fe4f00b13e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(10.199999809265137, 'Fred', 123, 70114101100)>\n",
      "<Row(18.5, 'Yeong', 234, 89101111110103)>\n"
     ]
    }
   ],
   "source": [
    "for x in rdd.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65e81f-8b82-4bff-bf61-f976be86b635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3acb5-53b8-436c-be48-ae8b463c1b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f65e4-41d4-4501-b881-24ca13014a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f8034-15e0-41a0-91b9-05b9b546be63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac0031bd-1c22-4c2d-bedf-3cd4c10df7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb61d77b-93a9-4326-8c2e-6a22936a51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb247a29-2188-4687-bb77-ad0b613aeca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Using UDF on SQL \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)\n",
    "     \n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
    "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9aeff0-5687-4c0f-8308-fbc4bf463f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f5cc4ce-d0c8-4ada-aae8-7c58d268829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a87423ac-0f7c-47c8-8446-770ca95a8746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91c52e95-84cb-4688-8c4f-9d8be16fb588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c47411-94e3-465e-8f82-186590103664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdf9a77-42f5-4c19-b786-290532b31285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "@pandas_udf(IntegerType()) # spark에 udf 등록\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe70ebc-77bc-44d5-bbae-8f6c45badb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import PandasUDFType\n",
    "@pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "def slen(s):\n",
    "    return s.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95f42a-6abe-4586-a9c2-731bb9a2beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()\n",
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461eb080-192c-4a79-a973-e3ffe9542f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16d3d6-79f6-4ca8-97f2-4cf6ebf86455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318ba9d-e2b7-4b0e-a0c7-db39d1cc163d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
