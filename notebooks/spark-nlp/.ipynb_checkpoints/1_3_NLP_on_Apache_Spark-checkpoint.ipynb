{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzgTx-7OuEnm"
   },
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "dgBCSO9Fr-M5",
    "outputId": "6ecefdc7-4cb1-4233-e898-0f397ff35813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/alex/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-07-28 16:29:49--  https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1860687 (1.8M) [application/x-httpd-php]\n",
      "Saving to: ‘mini_newsgroups.tar.gz.1’\n",
      "\n",
      "mini_newsgroups.tar 100%[===================>]   1.77M  5.41MB/s    in 0.3s    \n",
      "\n",
      "2020-07-28 16:29:49 (5.41 MB/s) - ‘mini_newsgroups.tar.gz.1’ saved [1860687/1860687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7DAiInZuIsG"
   },
   "outputs": [],
   "source": [
    "! tar xzf mini_newsgroups.tar.gz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "nAMhZwEW4M3A",
    "outputId": "1cd10ec4-ec0b-49df-cb8e-a278428f3fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/alex/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-07-28 16:29:52--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4551 (4.4K) [application/x-httpd-php]\n",
      "Saving to: ‘iris.data’\n",
      "\n",
      "iris.data           100%[===================>]   4.44K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2020-07-28 16:29:52 (2.31 MB/s) - ‘iris.data’ saved [4551/4551]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8N8Uqqj4MnM"
   },
   "outputs": [],
   "source": [
    "! mv iris.data ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YofQ9lTttiGS"
   },
   "source": [
    "## NLP on Apache Spark\n",
    "It's no longer news that there is a data deluge. Every day, people and devices are creating huge amounts of data. Text data is definitely one of the main kinds of data that humans produce. People write millions of comments, product reviews, Reddit messages, and tweets per day. This data is incredibly valuable—for both research and commerce. Because of the scale at which this data is created, our approach to working with it has changed.\n",
    "\n",
    "Most of the original research in NLP was done on small data sets with hundreds or thousands of documents. You may think that it would be easier to build NLP applications now that we have so much more text data with which to build better models. However, these pieces of text have different pragmatics and are of different varieties, so leveraging them is more complicated from a data-science perspective. From the software engineering perspective, big data introduces many challenges. Structured data has predictable size and organization, which makes it easier to store and distribute efficiently. Text data is much less consistent. This makes parallelizing and distributing work more important and potentially more complex. Distributed computing frameworks like Spark help us manage these challenges and complexities.\n",
    "\n",
    "In this chapter, we will discuss the Apache Spark and Spark NLP. First, we will cover some basic concepts that will help us understand distributed computing. Then, we will talk briefly about the history of distributed computing. We will talk about some important modules in Spark—Spark SQL and MLlib. This will give us the background and context needed to talk about Spark NLP in technical detail.\n",
    "\n",
    "Now, we'll cover some technical concepts that will be helpful in understanding how Spark works. The explanations will be high level. If you are interested in maximizing performance, I suggest looking more into these topics. For the general audience, I hope this material will give you the intuition necessary to help make decisions when designing and building Spark-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYAKO09Ztj1N"
   },
   "source": [
    "## Parallelism, Concurrency, Distributing Computation\n",
    "Let's start by defining some terms.  A process can be thought of as a running program.  A process executes its code using an allotted portion of memory also known as a memory space. A thread is a sequence of execution steps within a process that the operating system can schedule. Sharing data between processes generally requires copying data between the different memory spaces.  When a Java or Scala program is run, the Java Virtual Machine (JVM) is the process. The threads of a process share access to the same memory space, which they access concurrently.\n",
    "\n",
    " Concurrent access of data can be tricky. For example, let's say we want to generate word counts. If two threads are working on this process, it's possible for us to get the wrong count. Consider the following program (written in pseudo-Python). In this program, we will use a thread pool.  A thread pool is a way to separate partitioning work from scheduling. We allocate a certain number of threads, and then we go through our data asking for threads for the pool. The operating system can then schedule the work.\n",
    "\n",
    "```\n",
    "0:  def word_count(tokenized_documents): # list of lists of tokens\n",
    "1:      word_counts = {}\n",
    "2:      thread_pool = ThreadPool()\n",
    "3:      i = 0\n",
    "4:      for thread in thread_pool\n",
    "5:          run thread:\n",
    "6:              while i < len(tokenized_documents):\n",
    "7:                  doc = tokenized_documents[i]\n",
    "8:                  i += 1\n",
    "9:                  for token in doc:\n",
    "10:                     old_count = word_counts.get(token, 0)\n",
    "11:                     word_counts[token] = old_count + 1\n",
    "12:     return word_counts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ox34D_1Sto1f"
   },
   "source": [
    "This looks reasonable, but we see that the code under `run thread` references data in the shared memory space like `i` and `word_counts`. tableone shows the execution of this program with two `thread`s in the `ThreadPool` starting at line 6.\n",
    "\n",
    "_Two `threads` in the `ThreadPool`_\n",
    "\n",
    "time|thread1|thread2|i|valid_state\n",
    "----|-------|-------|-|-----------\n",
    "0|while i < len(tokenized_documents)||0|yes\n",
    "1||while i < len(tokenized_documents)|0|yes\n",
    "2|doc = tokenized_documents[i]||0|yes\n",
    "3||doc = tokenized_documents[i]|0|NO\n",
    "\n",
    "At time 3, `thread2` will be retrieving `tokenized_documents[0]`, while `thread1` is already set to work on the first document.  This program has a race condition, in which we can get incorrect results depending on the sequence of operations done in the different threads. Avoiding these problems generally involves writing code that is safe for concurrent access. For example, we can pause `thread2` until `thread1` is finished updating by  locking on `tokenized_documents`. If you look at the code, there is another race condition, on `i`. If thread1 takes the last document, `tokenized_documents[N-1]`, `thread2` starts its while-loop check, `thread1` updates `i`, then `thread2` uses `i`. We will be accessing `tokenized_documents[N]`, which doesn't exist. So let's lock on `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVdEHp-ltpCk"
   },
   "source": [
    "```\n",
    "0:  def word_count(tokenized_documents): # list of lists of tokens\n",
    "1:      word_counts = {}\n",
    "2:      thread_pool = ThreadPool()\n",
    "3:      i = 0\n",
    "4:      for thread in thread_pool\n",
    "5:          run thread:\n",
    "6:              while True:\n",
    "7:                  lock i:\n",
    "8:                      if i < len(tokenized_documents)\n",
    "9:                          doc = tokenized_documents[i]\n",
    "10:                         i += 1\n",
    "11:                     else:\n",
    "12:                         break\n",
    "13:                 for token in doc:\n",
    "14:                     lock word_counts:\n",
    "15:                         old_count = word_counts.get(token, 0)\n",
    "16:                         word_counts[token] = old_count + 1\n",
    "17:     return word_counts\n",
    "```\n",
    "\n",
    "Now, we are locking on `i` and checking `i` in the loop. We also lock on `word_counts` so that if two threads want to update the counts of the same word, they won't accidentally pull a stale value for `old_count`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJU_0w8atp3R"
   },
   "source": [
    "time|thread1|thread2|i|valid_state\n",
    "----|-------|-------|-|-----------\n",
    "0|lock i||0|yes\n",
    "1|if i < len(tokenized_documents)|blocked|0|yes\n",
    "2|doc = tokenized_documents[i]|blocked|0|yes\n",
    "3|i += 1|blocked|0|yes\n",
    "4||lock i|1|yes\n",
    "5|lock word_counts||1|yes\n",
    "6||if i < len(tokenized_documents)|1|yes\n",
    "7|old_count = word_counts.get(token, 0)||1|yes\n",
    "8||doc = tokenized_documents[i]|1|yes\n",
    "9|word_counts[token] = old_count + 1||1|yes\n",
    "10||i += 1|1|yes\n",
    "11|lock word_counts||2|yes\n",
    "12|old_count = word_counts.get(token, 0)|blocked|2|yes\n",
    "13|word_counts[token] = old_count + 1|blocked|2|yes\n",
    "14||lock word_counts|2|yes\n",
    "15|blocked\t|old_count = word_counts.get(token, 0)|2|yes\n",
    "16|blocked\t|word_counts[token] = old_count + 1|2|yes\n",
    "\n",
    "We fixed the problem, but at the cost of frequently blocking one of the threads. This means that we are getting less advantage of the parallelism. It would be better to design our algorithm so that the threads don't share state. We will see an example of this when we talk about MapReduce. \n",
    "\n",
    "Sometimes, parallelizing on one machine is not sufficient, so we distribute the work across many machines grouped together in a cluster. When all the work is done on a machine, we are bringing the data (in memory or on disk) to the code, but when distributing work we are bringing the code to the data. Distributing the work of a program across a cluster means we have new concerns. We don't have access to a shared memory space, so we need to be more thoughtful in how we design our algorithms. Although processes on different machines don't share a common memory space, we still need to consider concurrency because the threads of the process on a given machine of the cluster still share common (local) memory space. Fortunately, modern frameworks like Spark mostly take care of these concerns, but it's still good to keep this in mind when designing your programs.\n",
    "\n",
    "Programs that work on text data often find some form of parallelization helpful because processing the text into structured data is often the most time-consuming stage of a program. Most NLP pipelines ultimately output structured numeric data, which means that the data that's loaded, the text, can often be much larger than the data that is output. Unfortunately, because of the complexity of NLP algorithms, text processing in distributed frameworks is generally limited to basic techniques. Fortunately, we have Spark NLP, which we will discuss shortly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zracfS7tqtI"
   },
   "source": [
    "### Parallelization Before Apache Hadoop\n",
    "HTCondor is a framework developed at the University of Wisconsin–Madison starting in 1988. It boasts an impressive catalog of uses. It was used by NASA, the Human Genome Project, and the Large Hadron Collider. Technically, it's not just a framework for distributing computation—it also can manage resources. In fact, it can be used with other frameworks for distributing computation. It was built with the idea that machines in the cluster may be owned by different users, so work can be scheduled based on available resources. This is from a time when clusters of computers were not as available.\n",
    "\n",
    "GNU parallel and pexec are UNIX tools that can be used to parallelize work on a single machine, as well as across machines. This requires that the distributable portions of work be run from the command line. These tools allow us to utilize resources across machines, but it doesn't help with parallelizing our algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9G1uNCa_tq6p"
   },
   "source": [
    "### MapReduce and Apache Hadoop\n",
    "We can represent distributed computation with two operations: map and reduce. The map operation can be used to transform, filter, or sort data. The reduce operation can be used to group or summarize data. Let's return to our word count example to see how we can use these two operations for a basic NLP task.\n",
    "\n",
    "```\n",
    "def map(docs):\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            yield (token, 1)\n",
    "\n",
    "def reduce(records):\n",
    "    word_counts = {}\n",
    "    for token, count in records:\n",
    "        word_counts[token] = word_counts.get(token, 0) + count\n",
    "    for word, count in word_counts.items():\n",
    "        yield (word, count)\n",
    "```\n",
    "\n",
    "The data is loaded in partitions, with some documents going to each `mapper` process on the cluster. There can be multiple `mapper`s per machine. Each `mapper` runs the map function on their documents and saves the results to the disk. After all of the `mapper`s have completed, the data from the `mapper` stage is shuffled so that, all the records with the same key (`word` in this case), are in the same partition. This data is now sorted so that within a partition, all the records are ordered by key. Finally, the sorted data is loaded and the `reduce` step is called for each partition `reducer` process combining all of the counts. In between stages, the data is saved to the disk.\n",
    "\n",
    "MapReduce can express most distributed algorithms, but some are difficult or downright awkward in this framework. This is why abstractions to MapReduce were developed rather quickly.\n",
    "\n",
    "Apache Hadoop is the popular open source implementation of MapReduce along with a distributed file system,  Hadoop Distributed File System (HDFS). To write a Hadoop program, you need to select or define an input format, mapper, reducer, and output format. There have been many libraries and frameworks to allow higher levels of implementing a program.\n",
    "\n",
    " Apache Pig is a framework for expressing MapReduce programs in procedural code. Its procedural nature makes implementing extract, transform, load (ETL) programs very convenient and straightforward. However, other types of programs, model training programs for example, are much more difficult.  The language that Apache Pig uses is called Pig Latin. There is some overlap with SQL, so if someone knows SQL well, learning Pig Latin is easy.\n",
    "\n",
    " Apache Hive is a data warehousing framework originally built on Hadoop. Hive allows users to write SQL that is executed with MapReduce. Now, Hive can run using other distributed frameworks in addition to Hadoop, including Spark.\n",
    "\n",
    "### Apache Spark\n",
    "Spark is a project started by Matei Zaharia. Spark is a distributed computing framework. There are important differences in how Spark and Hadoop process data. Spark allows users to write arbitrary code against distributed data. Currently, there are official APIs for Spark in Scala, Java, Python, and R. Spark does not save intermediate data to disk. Usually, Spark-based programs will keep data in memory, though this can be changed by configuration to also utilize the disk. This allows for quicker processing but can require more scale out (more machines), or more scale up (machines with more memory).\n",
    "\n",
    "Let's look at `word_count` in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2VtYbneyysy"
   },
   "source": [
    "## Architecture of Apache Spark\n",
    "Spark is organized around a driver that is running the program, a master that manages resources and distributes work, and workers that execute computation. There are a number of possible masters. Spark ships with its own master, which is what is used in standalone and local modes. You can also use Apache YARN or Apache Mesos. In my experience, Apache YARN is the most common choice for enterprise systems.\n",
    "\n",
    "Let's take a more detailed look at Spark's architecture.\n",
    "\n",
    "### Physical Architecture\n",
    "We start our program on the submitting machine which submits an application. This driver runs the application on the client machine and sends jobs to the  spark master to be distributed to the workers. The spark master may not be a completely separate machine. That machine may also be doing work on the cluster and so would be a worker as well. Also, you may be running your program on the spark master, and so it could also be the client machine.\n",
    "\n",
    "There are two modes that you can use to start a Spark application: cluster mode and client mode. If the machine submitting the application is the same machine that runs the application, you are in client mode, because you are submitting from the client machine. Otherwise, you are in cluster mode. Generally, you use client mode if your machine is inside the cluster and cluster mode if it is not (see Figures #client and #cluster).\n",
    "\n",
    "![Physical architecture (client mode)](https://i.imgur.com/ZiuOmLo.png)  \n",
    "_Physical architecture (client mode)_\n",
    "\n",
    "![Physical architecture (cluster mode)](https://i.imgur.com/lMltdz3.png)  \n",
    "_Physical architecture (cluster mode)_\n",
    "\n",
    "You can also run Spark in local mode in which, as the name implies, client machine, spark master, and worker are all the same machine. This is very useful for developing and testing Spark applications. It is also useful if you want to parallelize work on one machine.\n",
    "\n",
    "Now that we have an idea of the physical architecture, let's look at the logical architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BG0sLPj3yzI_"
   },
   "source": [
    "### Logical Architecture\n",
    "\n",
    "In looking at the logical architecture of Spark, we will treat the client machine, spark master, and worker as if they are different (see architecture). The driver is a JVM process that will submit work to the spark master. If the program is a Java or Scala program, then it is also the process running the program. If the program is in Python or R, then the driver process is a separate process from that running the program.\n",
    "\n",
    "![Logical architecture](https://i.imgur.com/WaM6zvN.png)  \n",
    "_Logical architecture_\n",
    "\n",
    "The JVM processes on the workers are called executors. The work to be done is defined on the driver and submitted to the spark master, which orchestrates the executors to do the work. The next step in understanding Spark is understanding how the data is distributed.\n",
    "\n",
    "#### RDDs\n",
    "\n",
    "Spark distributes data in resilient distributed datasets (RDDs). RDDs allow users to work with distributed data almost as if it were a collection located in the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sjUgZQXyzNn"
   },
   "source": [
    "#### Partitioning\n",
    "\n",
    "In Spark, data is partitioned across the cluster. There are usually more partitions than executors. This allows for each thread on each executor to be utilized. Spark will distribute the data in the RDD evenly across the cluster into the default number of partitions. We can specify the number of partitions, and we can specify a field to partition by. This can be very useful when your algorithm requires some degree of locality—for example, having all the tweets from a single user.\n",
    "\n",
    "#### Serialization\n",
    "Any code that is shipped to the data should refer only to serializable objects. `NotSerializableException` errors are common and can be nearly inscrutable to those new to Spark.  When we map over an `RDD`, we are creating a `Function` and sending it to the machines with the data. A function is the code that defines it, and the data needed in the definition. This second part is called the closure of the function. Identifying what objects are needed for a function is a complicated task, and sometimes extraneous objects can be captured. If you are having problems with serializability, there are a couple of possible solutions. The following are questions that can help you find the right solution:\n",
    "\n",
    "* Are you using your own custom classes? Make sure that they are serializable.\n",
    "* Are you loading a resource? Perhaps your distributed code should load it lazily so that it is loaded on each executor, instead of being loaded on the driver and shipped to the executors.\n",
    "* Are Spark objects (`SparkSession`, `RDD`s) being captured in a closure? This can happen when you are defining your function anonymously. If your function is defined anonymously, perhaps you can define it elsewhere.\n",
    "\n",
    "These tips can help find common errors, but the solution to this problem is something that can be determined only on a case-by-case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L32XtqkiyzSS"
   },
   "source": [
    "#### Ordering\n",
    "\n",
    "When working with distributed data, there is not necessarily a guaranteed order to the items in your data. When writing your code, keep in mind that the data exists in partitions across the cluster.\n",
    "\n",
    "This is not to say that we cannot define an order. We can define an order on the partitions by an index. In this situation, the \"first\" element of the `RDD` will be the first element of the first partition. Furthermore, let's say we want to order an `RDD[Int]` ascending by value. Using our ordering on partitions, we can shuffle the data such that all elements in partition `i` are less than all elements in partition `i+1`. From here, we can sort each partition. Now we have a sorted `RDD`. This is an expensive operation, however.\n",
    "\n",
    "#### Output and logging\n",
    "\n",
    "When writing functions that are used to transform data, it is often useful to print statements or, preferably, to log statements to look at the state of variables in the function. In a distributed context, this is more complicated because the function is not running on the same machine as the program. Accessing the logs and the stdout generally depends on the configuration of the cluster and which master you are using. In some situations, it may suffice to run your program on a small set of data in local mode.\n",
    "\n",
    "#### Spark jobs\n",
    "\n",
    "A Spark-based program will have a `SparkSession`, which is how the driver talks to the master.  Before Spark version 2.x, the `SparkContext` was used for this. There is still a `SparkContext`, but it is part of the `SparkSession` now. This `SparkSession` represents the `App`. The `App` submits `jobs` to the master. The `jobs` are broken into `stages`, which are logical groupings of work in the job. The `stages` are broken into `tasks`, which represent the work to be done on each partition.\n",
    "\n",
    "![Spark jobs](https://i.imgur.com/Lmfc5Nc.png)  \n",
    "_Spark jobs_\n",
    "\n",
    "Not every operation on data will start a job. Spark is lazy—in a good way. Execution is done only when a result is needed. This allows the work done to be organized into an execution plan to improve efficiency. There are certain operations, sometimes referred to as actions, that will cause execution immediately. These are operations that return specific values, e.g. `aggregate`. There are also operations where further execution planning becomes impossible until computed, e.g. `zipWithIndex`.\n",
    "\n",
    "  This execution plan is often referred to as the Directed Acyclic Graph (DAG). Data in Spark is defined by its DAG, which includes its sources and whatever operations were run to generate it. This allows Spark to remove data from memory as necessary without losing reference to the data. If data that was generated and removed is referred to later, it will be regenerated. This can be time-consuming. Fortunately, we can instruct Spark to keep data if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVDozVzNyzWE"
   },
   "source": [
    "#### Persisting\n",
    "\n",
    "The basic way of persisting data in Spark is with the persist method. This creates a checkpoint. You can also use the cache method and provide options for configuring how the data will be persisted. The data will still be generated lazily, but once it is generated, it will be kept.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT9sA32I2uxJ"
   },
   "outputs": [],
   "source": [
    "from operator import concat, itemgetter, methodcaller\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hk7cOg82wxl"
   },
   "outputs": [],
   "source": [
    "packages = ','.join([\n",
    "    \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\",\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark-nlp-book-p1c3\") \\\n",
    "    .config(\"spark.jars.packages\", packages)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTnaiFGd2zr2"
   },
   "outputs": [],
   "source": [
    "def has_moon(text):\n",
    "    if 'moon' in text:\n",
    "        sleep(1)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "15Rps9or1UgZ",
    "outputId": "7697f362-c7e7-4b78-a5aa-86416bf4847d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This appears quickly because the previous operations are all lazy\n",
      "11\n",
      "This appears slowly since the count method will call has_moon which sleeps\n",
      "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohi\n",
      "This appears quickly because has_moon will not be called due to the data being persisted\n"
     ]
    }
   ],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "path = os.path.join('data', 'mini_newsgroups', 'sci.space')\n",
    "text_pairs = spark.sparkContext\\\n",
    "    .wholeTextFiles(path) \n",
    "texts = text_pairs.map(itemgetter(1))\n",
    "lower_cased = texts.map(methodcaller('lower'))\n",
    "moon_texts = texts.filter(has_moon).persist()\n",
    "print('This appears quickly because the previous operations are '\n",
    "      'all lazy')\n",
    "print(moon_texts.count())\n",
    "print('This appears slowly since the count method will call '\n",
    "      'has_moon which sleeps')\n",
    "print(moon_texts.reduce(concat)[:100])\n",
    "print('This appears quickly because has_moon will not be '\n",
    "      'called due to the data being persisted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rItFYkup28xs"
   },
   "source": [
    "Now that we have an idea of how Spark works, let's go back to our word-count problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeQtbQ2_1Wll"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import add\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "kvb4Iavz2_1z",
    "outputId": "c32035d9-024a-4ad3-f43c-eee6fd469990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "filepath-text pair of first document\n",
      "('file:/home/alex/projects/spark-nlp-book-prod/jupyter/data/mini_newsgroups/sci.space/59848', 'Xref: cantaloupe.srv.cs.cmu.edu sci.space:59848 sci.answers:100 news.answers:7198\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!ogicse!decwrl!concert!borg.cs.unc.edu!not-for-mail\\nFrom: leech@cs.unc.edu (Jon Leech)\\nNewsgroups: sci.space,sci.answers,news.answers\\nSubject: Space FAQ 12/15 - Controversial Questions\\nKeywords: Frequently Asked Questions\\nMessage-ID: <controversy_733694426@cs.unc.edu>\\nDate: 1 Apr 93 10:00:29 GMT\\nArticle-I.D.: cs.controversy_733694426\\nExpires: 6 May 1993 20:00:26 GMT\\nReferences: <diffs_733693816@cs.unc.edu>\\nFollowup-To: poster\\nDistribution: world\\nOrganization: University of North Carolina, Chapel Hill\\nLines: 252\\nApproved: news-answers-request@MIT.Edu\\nSupersedes: <controversy_730956589@cs.unc.edu>\\nNNTP-Posting-Host: mahler.cs.unc.edu\\n\\nArchive-name: space/controversy\\nLast-modified: $Date: 93/04/01 14:39:06 $\\n\\nCONTROVERSIAL QUESTIONS\\n\\n    These issues periodically come up with much argument and few facts being\\n    offered. The summaries below attempt to represent the position on which\\n    much of the net community has settled. Please DON\\'T bring them up again\\n    unless there\\'s something truly new to be discussed. The net can\\'t set\\n    public policy, that\\'s what your representatives are for.\\n\\n\\n    WHAT HAPPENED TO THE SATURN V PLANS\\n\\n    Despite a widespread belief to the contrary, the Saturn V blueprints\\n    have not been lost. They are kept at Marshall Space Flight Center on\\n    microfilm.\\n\\n    The problem in re-creating the Saturn V is not finding the drawings, it\\n    is finding vendors who can supply mid-1960\\'s vintage hardware (like\\n    guidance system components), and the fact that the launch pads and VAB\\n    have been converted to Space Shuttle use, so you have no place to launch\\n    from.\\n\\n    By the time you redesign to accommodate available hardware and re-modify\\n    the launch pads, you may as well have started from scratch with a clean\\n    sheet design.\\n\\n\\n    WHY DATA FROM SPACE MISSIONS ISN\\'T IMMEDIATELY AVAILABLE\\n\\n    Investigators associated with NASA missions are allowed exclusive access\\n    for one year after the data is obtained in order to give them an\\n    opportunity to analyze the data and publish results without being\\n    \"scooped\" by people uninvolved in the mission. However, NASA frequently\\n    releases examples (in non-digital form, e.g. photos) to the public early\\n    in a mission.\\n\\n\\n    RISKS OF NUCLEAR (RTG) POWER SOURCES FOR SPACE PROBES\\n\\n    There has been extensive discussion on this topic sparked by attempts to\\n    block the Galileo and Ulysses launches on grounds of the plutonium\\n    thermal sources being dangerous. Numerous studies claim that even in\\n    worst-case scenarios (shuttle explosion during launch, or accidental\\n    reentry at interplanetary velocities), the risks are extremely small.\\n    Two interesting data points are (1) The May 1968 loss of two SNAP 19B2\\n    RTGs, which landed intact in the Pacific Ocean after a Nimbus B weather\\n    satellite failed to reach orbit. The fuel was recovered after 5 months\\n    with no release of plutonium. (2) In April 1970, the Apollo 13 lunar\\n    module reentered the atmosphere and its SNAP 27 RTG heat source, which\\n    was jettisoned, fell intact into the 20,000 feet deep Tonga Trench in\\n    the Pacific Ocean. The corrosion resistant materials of the RTG are\\n    expected to prevent release of the fuel for a period of time equal to 10\\n    half-lives of the Pu-238 fuel or about 870 years [DOE 1980].\\n\\n    To make your own informed judgement, some references you may wish to\\n    pursue are:\\n\\n    A good review of the technical facts and issues is given by Daniel\\n    Salisbury in \"Radiation Risk and Planetary Exploration-- The RTG\\n    Controversy,\" *Planetary Report*, May-June 1987, pages 3-7. Another good\\n    article, which also reviews the events preceding Galileo\\'s launch,\\n    \"Showdown at Pad 39-B,\" by Robert G. Nichols, appeared in the November\\n    1989 issue of *Ad Astra*. (Both magazines are published by pro-space\\n    organizations, the Planetary Society and the National Space Society\\n    respectively.)\\n\\n    Gordon L Chipman, Jr., \"Advanced Space Nuclear Systems\" (AAS 82-261), in\\n    *Developing the Space Frontier*, edited by Albert Naumann and Grover\\n    Alexander, Univelt, 1983, p. 193-213.\\n\\n    \"Hazards from Plutonium Toxicity\", by Bernard L. Cohen, Health Physics,\\n    Vol 32 (may) 1977, page 359-379.\\n\\n    NUS Corporation, Safety Status Report for the Ulysses Mission: Risk\\n    Analysis (Book 1). Document number is NUS 5235; there is no GPO #;\\n    published Jan 31, 1990.\\n\\n    NASA Office of Space Science and Applications, *Final Environmental\\n    Impact Statement for the Ulysses Mission (Tier 2)*, (no serial number or\\n    GPO number, but probably available from NTIS or NASA) June 1990.\\n\\n    [DOE 1980] U.S.  Department of Energy, *Transuranic Elements in the\\n    Environment*, Wayne C.  Hanson, editor; DOE Document No.  DOE/TIC-22800;\\n    Government Printing Office, Washington, D.C., April 1980.)\\n\\n\\n    IMPACT OF THE SPACE SHUTTLE ON THE OZONE LAYER\\n\\n    From time to time, claims are made that chemicals released from\\n    the Space Shuttle\\'s Solid Rocket Boosters (SRBs) are responsible\\n    for a significant amount of damage to the ozone layer. Studies\\n    indicate that they in reality have only a minute impact, both in\\n    absolute terms and relative to other chemical sources. The\\n    remainder of this item is a response from the author of the quoted\\n    study, Charles Jackman.\\n\\n    The atmospheric modelling study of the space shuttle effects on the\\n    stratosphere involved three independent theoretical groups, and was\\n    organized by Dr. Michael Prather, NASA/Goddard Institute for Space\\n    Studies.  The three groups involved Michael Prather and Maria Garcia\\n    (NASA/GISS), Charlie Jackman and Anne Douglass (NASA/Goddard Space\\n    Flight Center), and Malcolm Ko and Dak Sze (Atmospheric and\\n    Environmental Research, Inc.).  The effort was to look at the effects\\n    of the space shuttle and Titan rockets on the stratosphere.\\n\\n    The following are the estimated sources of stratospheric chlorine:\\n\\n       Industrial sources:    300,000,000 kilograms/year\\n\\t  Natural sources:     75,000,000 kilograms/year\\n\\t  Shuttle sources:\\t  725,000 kilograms/year\\n\\n    The shuttle source assumes 9 space shuttles and 6 Titan rockets are\\n    launched yearly. Thus the launches would add less than 0.25% to the\\n    total stratospheric chlorine sources.\\n\\n    The effect on ozone is minimal:  global yearly average total ozone would\\n    be decreased by 0.0065%. This is much less than total ozone variability\\n    associated with volcanic activity and solar flares.\\n\\n    The influence of human-made chlorine products on ozone is computed\\n    by atmospheric model calculations to be a 1% decrease in globally\\n    averaged ozone between 1980 and 1990. The influence of the space shuttle and\\n    Titan rockets on the stratosphere is negligible.  The launch\\n    schedule of the Space Shuttle and Titan rockets would need to be\\n    increased by over a factor of a hundred in order to have about\\n    the same effect on ozone as our increases in industrial halocarbons\\n    do at the present time.\\n\\n    Theoretical results of this study have been published in _The Space\\n    Shuttle\\'s Impact on the Stratosphere_, MJ Prather, MM Garcia, AR\\n    Douglass, CH Jackman, M.K.W. Ko and N.D. Sze, Journal of Geophysical\\n    Research, 95, 18583-18590, 1990.\\n\\n    Charles Jackman, Atmospheric Chemistry and Dynamics Branch,\\n    Code 916, NASA/Goddard Space Flight Center,\\n    Greenbelt, MD  20771\\n\\n    Also see _Chemical Rockets and the Environment_, A McDonald, R Bennett,\\n    J Hinshaw, and M Barnes, Aerospace America, May 1991.\\n\\n\\n    HOW LONG CAN A HUMAN LIVE UNPROTECTED IN SPACE\\n\\n    If you *don\\'t* try to hold your breath, exposure to space for half a\\n    minute or so is unlikely to produce permanent injury. Holding your\\n    breath is likely to damage your lungs, something scuba divers have to\\n    watch out for when ascending, and you\\'ll have eardrum trouble if your\\n    Eustachian tubes are badly plugged up, but theory predicts -- and animal\\n    experiments confirm -- that otherwise, exposure to vacuum causes no\\n    immediate injury. You do not explode. Your blood does not boil. You do\\n    not freeze. You do not instantly lose consciousness.\\n\\n    Various minor problems (sunburn, possibly \"the bends\", certainly some\\n    [mild, reversible, painless] swelling of skin and underlying tissue)\\n    start after ten seconds or so. At some point you lose consciousness from\\n    lack of oxygen. Injuries accumulate. After perhaps one or two minutes,\\n    you\\'re dying. The limits are not really known.\\n\\n    References:\\n\\n    _The Effect on the Chimpanzee of Rapid Decompression to a Near Vacuum_,\\n    Alfred G. Koestler ed., NASA CR-329 (Nov 1965).\\n\\n    _Experimental Animal Decompression to a Near Vacuum Environment_, R.W.\\n    Bancroft, J.E. Dunn, eds, Report SAM-TR-65-48 (June 1965), USAF School\\n    of Aerospace Medicine, Brooks AFB, Texas.\\n\\n\\n    HOW THE CHALLENGER ASTRONAUTS DIED\\n\\n    The Challenger shuttle launch was not destroyed in an explosion. This is\\n    a well-documented fact; see the Rogers Commission report, for example.\\n    What looked like an explosion was fuel burning after the external tank\\n    came apart. The forces on the crew cabin were not sufficient to kill the\\n    astronauts, never mind destroy their bodies, according to the Kerwin\\n    team\\'s medical/forensic report.\\n\\n    The astronauts were killed when the more-or-less intact cabin hit the\\n    water at circa 200MPH, and their bodies then spent several weeks\\n    underwater. Their remains were recovered, and after the Kerwin team\\n    examined them, they were sent off to be buried.\\n\\n\\n    USING THE SHUTTLE BEYOND LOW EARTH ORBIT\\n\\n    You can\\'t use the shuttle orbiter for missions beyond low Earth orbit\\n    because it can\\'t get there. It is big and heavy and does not carry\\n    enough fuel, even if you fill part of the cargo bay with tanks.\\n\\n    Furthermore, it is not particularly sensible to do so, because much of\\n    that weight is things like wings, which are totally useless except in\\n    the immediate vicinity of the Earth. The shuttle orbiter is highly\\n    specialized for travel between Earth\\'s surface and low orbit. Taking it\\n    higher is enormously costly and wasteful. A much better approach would\\n    be to use shuttle subsystems to build a specialized high-orbit\\n    spacecraft.\\n\\n    [Yet another concise answer by Henry Spencer.]\\n\\n\\n    THE \"FACE ON MARS\"\\n\\n    There really is a big rock on Mars that looks remarkably like a humanoid\\n    face. It appears in two different frames of Viking Orbiter imagery:\\n    35A72 (much more facelike in appearance, and the one more often\\n    published, with the Sun 10 degrees above western horizon) and 70A13\\n    (with the Sun 27 degrees from the west).\\n\\n    Science writer Richard Hoagland has championed the idea that the Face is\\n    artificial, intended to resemble a human, and erected by an\\n    extraterrestrial civilization. Most other analysts concede that the\\n    resemblance is most likely accidental. Other Viking images show a\\n    smiley-faced crater and a lava flow resembling Kermit the Frog elsewhere\\n    on Mars. There exists a Mars Anomalies Research Society (sorry, don\\'t\\n    know the address) to study the Face.\\n\\n    The Mars Observer mission will carry an extremely high-resolution\\n    camera, and better images of the formation will hopefully settle this\\n    question in a few years. In the meantime, speculation about the Face is\\n    best carried on in the altnet group alt.alien.visitors, not sci.space or\\n    sci.astro.\\n\\n    V. DiPeitro and G. Molenaar, *Unusual Martian Surface Features*, Mars\\n    Research, P.O. Box 284, Glen Dale, Maryland, USA, 1982. $18 by mail.\\n\\n    R.R. Pozos, *The Face of Mars*, Chicago Review Press, 1986. [Account of\\n    an interdisciplinary speculative conference Hoagland organized to\\n    investigate the Face]\\n\\n    R.C. Hoagland, *The Monuments of Mars: A City on the Edge of Forever*,\\n    North Atlantic Books, Berkeley, California, USA, 1987. [Elaborate\\n    discussion of evidence and speculation that formations near the Face\\n    form a city]\\n\\n    M.J. Carlotto, \"Digital Imagery Analysis of Unusual Martian Surface\\n    Features,\" *Applied Optics*, 27, pp. 1926-1933, 1987. [Extracts\\n    three-dimensional model for the Face from the 2-D images]\\n\\n    M.J. Carlotto & M.C. Stein, \"A Method of Searching for Artificial\\n    Objects on Planetary Surfaces,\" *Journal of the British Interplanetary\\n    Society*, Vol. 43 no. 5 (May 1990), p.209-216. [Uses a fractal image\\n    analysis model to guess whether the Face is artificial]\\n\\n    B. O\\'Leary, \"Analysis of Images of the `Face\\' on Mars and Possible\\n    Intelligent Origin,\" *JBIS*, Vol. 43 no. 5 (May 1990), p. 203-208.\\n    [Lights Carlotto\\'s model from the two angles and shows it\\'s consistent;\\n    shows that the Face doesn\\'t look facelike if observed from the surface]\\n\\n\\nNEXT: FAQ #13/15 - Space activist/interest/research groups & space publications\\n')\n"
     ]
    }
   ],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "texts = spark.sparkContext.wholeTextFiles(path) \n",
    "print('\\n\\nfilepath-text pair of first document')\n",
    "print(texts.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "99l4bgDe3BV5",
    "outputId": "ac24662f-5601-4b02-f509-14a47684f2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tokenized text of first document\n",
      "['Xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'sci', 'space', '59848', 'sci', 'answers', '100', 'news', 'answers', '7198', 'Path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'ogicse', 'decwrl', 'concert', 'borg', 'cs', 'unc', 'edu', 'not', 'for', 'mail', 'From', 'leech', 'cs', 'unc', 'edu', 'Jon', 'Leech', 'Newsgroups', 'sci', 'space', 'sci', 'answers', 'news', 'answers', 'Subject', 'Space', 'FAQ', '12', '15', 'Controversial', 'Questions', 'Keywords', 'Frequently', 'Asked', 'Questions', 'Message', 'ID', 'controversy_733694426', 'cs', 'unc', 'edu', 'Date', '1', 'Apr', '93', '10', '00', '29', 'GMT', 'Article', 'I', 'D', 'cs', 'controversy_733694426', 'Expires', '6', 'May', '1993', '20', '00', '26', 'GMT', 'References', 'diffs_733693816', 'cs', 'unc', 'edu', 'Followup', 'To', 'poster', 'Distribution', 'world', 'Organization', 'University', 'of', 'North', 'Carolina', 'Chapel', 'Hill', 'Lines', '252', 'Approved', 'news', 'answers', 'request', 'MIT', 'Edu', 'Supersedes', 'controversy_730956589', 'cs', 'unc', 'edu', 'NNTP', 'Posting', 'Host', 'mahler', 'cs', 'unc', 'edu', 'Archive', 'name', 'space', 'controversy', 'Last', 'modified', 'Date', '93', '04', '01', '14', '39', '06', 'CONTROVERSIAL', 'QUESTIONS', 'These', 'issues', 'periodically', 'come', 'up', 'with', 'much', 'argument', 'and', 'few', 'facts', 'being', 'offered', 'The', 'summaries', 'below', 'attempt', 'to', 'represent', 'the', 'position', 'on', 'which', 'much', 'of', 'the', 'net', 'community', 'has', 'settled', 'Please', 'DON', 'T', 'bring', 'them', 'up', 'again', 'unless', 'there', 's', 'something', 'truly', 'new', 'to', 'be', 'discussed', 'The', 'net', 'can', 't', 'set', 'public', 'policy', 'that', 's', 'what', 'your', 'representatives', 'are', 'for', 'WHAT', 'HAPPENED', 'TO', 'THE', 'SATURN', 'V', 'PLANS', 'Despite', 'a', 'widespread', 'belief', 'to', 'the', 'contrary', 'the', 'Saturn', 'V', 'blueprints', 'have', 'not', 'been', 'lost', 'They', 'are', 'kept', 'at', 'Marshall', 'Space', 'Flight', 'Center', 'on', 'microfilm', 'The', 'problem', 'in', 're', 'creating', 'the', 'Saturn', 'V', 'is', 'not', 'finding', 'the', 'drawings', 'it', 'is', 'finding', 'vendors', 'who', 'can', 'supply', 'mid', '1960', 's', 'vintage', 'hardware', 'like', 'guidance', 'system', 'components', 'and', 'the', 'fact', 'that', 'the', 'launch', 'pads', 'and', 'VAB', 'have', 'been', 'converted', 'to', 'Space', 'Shuttle', 'use', 'so', 'you', 'have', 'no', 'place', 'to', 'launch', 'from', 'By', 'the', 'time', 'you', 'redesign', 'to', 'accommodate', 'available', 'hardware', 'and', 're', 'modify', 'the', 'launch', 'pads', 'you', 'may', 'as', 'well', 'have', 'started', 'from', 'scratch', 'with', 'a', 'clean', 'sheet', 'design', 'WHY', 'DATA', 'FROM', 'SPACE', 'MISSIONS', 'ISN', 'T', 'IMMEDIATELY', 'AVAILABLE', 'Investigators', 'associated', 'with', 'NASA', 'missions', 'are', 'allowed', 'exclusive', 'access', 'for', 'one', 'year', 'after', 'the', 'data', 'is', 'obtained', 'in', 'order', 'to', 'give', 'them', 'an', 'opportunity', 'to', 'analyze', 'the', 'data', 'and', 'publish', 'results', 'without', 'being', 'scooped', 'by', 'people', 'uninvolved', 'in', 'the', 'mission', 'However', 'NASA', 'frequently', 'releases', 'examples', 'in', 'non', 'digital', 'form', 'e', 'g', 'photos', 'to', 'the', 'public', 'early', 'in', 'a', 'mission', 'RISKS', 'OF', 'NUCLEAR', 'RTG', 'POWER', 'SOURCES', 'FOR', 'SPACE', 'PROBES', 'There', 'has', 'been', 'extensive', 'discussion', 'on', 'this', 'topic', 'sparked', 'by', 'attempts', 'to', 'block', 'the', 'Galileo', 'and', 'Ulysses', 'launches', 'on', 'grounds', 'of', 'the', 'plutonium', 'thermal', 'sources', 'being', 'dangerous', 'Numerous', 'studies', 'claim', 'that', 'even', 'in', 'worst', 'case', 'scenarios', 'shuttle', 'explosion', 'during', 'launch', 'or', 'accidental', 'reentry', 'at', 'interplanetary', 'velocities', 'the', 'risks', 'are', 'extremely', 'small', 'Two', 'interesting', 'data', 'points', 'are', '1', 'The', 'May', '1968', 'loss', 'of', 'two', 'SNAP', '19B2', 'RTGs', 'which', 'landed', 'intact', 'in', 'the', 'Pacific', 'Ocean', 'after', 'a', 'Nimbus', 'B', 'weather', 'satellite', 'failed', 'to', 'reach', 'orbit', 'The', 'fuel', 'was', 'recovered', 'after', '5', 'months', 'with', 'no', 'release', 'of', 'plutonium', '2', 'In', 'April', '1970', 'the', 'Apollo', '13', 'lunar', 'module', 'reentered', 'the', 'atmosphere', 'and', 'its', 'SNAP', '27', 'RTG', 'heat', 'source', 'which', 'was', 'jettisoned', 'fell', 'intact', 'into', 'the', '20', '000', 'feet', 'deep', 'Tonga', 'Trench', 'in', 'the', 'Pacific', 'Ocean', 'The', 'corrosion', 'resistant', 'materials', 'of', 'the', 'RTG', 'are', 'expected', 'to', 'prevent', 'release', 'of', 'the', 'fuel', 'for', 'a', 'period', 'of', 'time', 'equal', 'to', '10', 'half', 'lives', 'of', 'the', 'Pu', '238', 'fuel', 'or', 'about', '870', 'years', 'DOE', '1980', 'To', 'make', 'your', 'own', 'informed', 'judgement', 'some', 'references', 'you', 'may', 'wish', 'to', 'pursue', 'are', 'A', 'good', 'review', 'of', 'the', 'technical', 'facts', 'and', 'issues', 'is', 'given', 'by', 'Daniel', 'Salisbury', 'in', 'Radiation', 'Risk', 'and', 'Planetary', 'Exploration', 'The', 'RTG', 'Controversy', 'Planetary', 'Report', 'May', 'June', '1987', 'pages', '3', '7', 'Another', 'good', 'article', 'which', 'also', 'reviews', 'the', 'events', 'preceding', 'Galileo', 's', 'launch', 'Showdown', 'at', 'Pad', '39', 'B', 'by', 'Robert', 'G', 'Nichols', 'appeared', 'in', 'the', 'November', '1989', 'issue', 'of', 'Ad', 'Astra', 'Both', 'magazines', 'are', 'published', 'by', 'pro', 'space', 'organizations', 'the', 'Planetary', 'Society', 'and', 'the', 'National', 'Space', 'Society', 'respectively', 'Gordon', 'L', 'Chipman', 'Jr', 'Advanced', 'Space', 'Nuclear', 'Systems', 'AAS', '82', '261', 'in', 'Developing', 'the', 'Space', 'Frontier', 'edited', 'by', 'Albert', 'Naumann', 'and', 'Grover', 'Alexander', 'Univelt', '1983', 'p', '193', '213', 'Hazards', 'from', 'Plutonium', 'Toxicity', 'by', 'Bernard', 'L', 'Cohen', 'Health', 'Physics', 'Vol', '32', 'may', '1977', 'page', '359', '379', 'NUS', 'Corporation', 'Safety', 'Status', 'Report', 'for', 'the', 'Ulysses', 'Mission', 'Risk', 'Analysis', 'Book', '1', 'Document', 'number', 'is', 'NUS', '5235', 'there', 'is', 'no', 'GPO', 'published', 'Jan', '31', '1990', 'NASA', 'Office', 'of', 'Space', 'Science', 'and', 'Applications', 'Final', 'Environmental', 'Impact', 'Statement', 'for', 'the', 'Ulysses', 'Mission', 'Tier', '2', 'no', 'serial', 'number', 'or', 'GPO', 'number', 'but', 'probably', 'available', 'from', 'NTIS', 'or', 'NASA', 'June', '1990', 'DOE', '1980', 'U', 'S', 'Department', 'of', 'Energy', 'Transuranic', 'Elements', 'in', 'the', 'Environment', 'Wayne', 'C', 'Hanson', 'editor', 'DOE', 'Document', 'No', 'DOE', 'TIC', '22800', 'Government', 'Printing', 'Office', 'Washington', 'D', 'C', 'April', '1980', 'IMPACT', 'OF', 'THE', 'SPACE', 'SHUTTLE', 'ON', 'THE', 'OZONE', 'LAYER', 'From', 'time', 'to', 'time', 'claims', 'are', 'made', 'that', 'chemicals', 'released', 'from', 'the', 'Space', 'Shuttle', 's', 'Solid', 'Rocket', 'Boosters', 'SRBs', 'are', 'responsible', 'for', 'a', 'significant', 'amount', 'of', 'damage', 'to', 'the', 'ozone', 'layer', 'Studies', 'indicate', 'that', 'they', 'in', 'reality', 'have', 'only', 'a', 'minute', 'impact', 'both', 'in', 'absolute', 'terms', 'and', 'relative', 'to', 'other', 'chemical', 'sources', 'The', 'remainder', 'of', 'this', 'item', 'is', 'a', 'response', 'from', 'the', 'author', 'of', 'the', 'quoted', 'study', 'Charles', 'Jackman', 'The', 'atmospheric', 'modelling', 'study', 'of', 'the', 'space', 'shuttle', 'effects', 'on', 'the', 'stratosphere', 'involved', 'three', 'independent', 'theoretical', 'groups', 'and', 'was', 'organized', 'by', 'Dr', 'Michael', 'Prather', 'NASA', 'Goddard', 'Institute', 'for', 'Space', 'Studies', 'The', 'three', 'groups', 'involved', 'Michael', 'Prather', 'and', 'Maria', 'Garcia', 'NASA', 'GISS', 'Charlie', 'Jackman', 'and', 'Anne', 'Douglass', 'NASA', 'Goddard', 'Space', 'Flight', 'Center', 'and', 'Malcolm', 'Ko', 'and', 'Dak', 'Sze', 'Atmospheric', 'and', 'Environmental', 'Research', 'Inc', 'The', 'effort', 'was', 'to', 'look', 'at', 'the', 'effects', 'of', 'the', 'space', 'shuttle', 'and', 'Titan', 'rockets', 'on', 'the', 'stratosphere', 'The', 'following', 'are', 'the', 'estimated', 'sources', 'of', 'stratospheric', 'chlorine', 'Industrial', 'sources', '300', '000', '000', 'kilograms', 'year', 'Natural', 'sources', '75', '000', '000', 'kilograms', 'year', 'Shuttle', 'sources', '725', '000', 'kilograms', 'year', 'The', 'shuttle', 'source', 'assumes', '9', 'space', 'shuttles', 'and', '6', 'Titan', 'rockets', 'are', 'launched', 'yearly', 'Thus', 'the', 'launches', 'would', 'add', 'less', 'than', '0', '25', 'to', 'the', 'total', 'stratospheric', 'chlorine', 'sources', 'The', 'effect', 'on', 'ozone', 'is', 'minimal', 'global', 'yearly', 'average', 'total', 'ozone', 'would', 'be', 'decreased', 'by', '0', '0065', 'This', 'is', 'much', 'less', 'than', 'total', 'ozone', 'variability', 'associated', 'with', 'volcanic', 'activity', 'and', 'solar', 'flares', 'The', 'influence', 'of', 'human', 'made', 'chlorine', 'products', 'on', 'ozone', 'is', 'computed', 'by', 'atmospheric', 'model', 'calculations', 'to', 'be', 'a', '1', 'decrease', 'in', 'globally', 'averaged', 'ozone', 'between', '1980', 'and', '1990', 'The', 'influence', 'of', 'the', 'space', 'shuttle', 'and', 'Titan', 'rockets', 'on', 'the', 'stratosphere', 'is', 'negligible', 'The', 'launch', 'schedule', 'of', 'the', 'Space', 'Shuttle', 'and', 'Titan', 'rockets', 'would', 'need', 'to', 'be', 'increased', 'by', 'over', 'a', 'factor', 'of', 'a', 'hundred', 'in', 'order', 'to', 'have', 'about', 'the', 'same', 'effect', 'on', 'ozone', 'as', 'our', 'increases', 'in', 'industrial', 'halocarbons', 'do', 'at', 'the', 'present', 'time', 'Theoretical', 'results', 'of', 'this', 'study', 'have', 'been', 'published', 'in', '_The', 'Space', 'Shuttle', 's', 'Impact', 'on', 'the', 'Stratosphere_', 'MJ', 'Prather', 'MM', 'Garcia', 'AR', 'Douglass', 'CH', 'Jackman', 'M', 'K', 'W', 'Ko', 'and', 'N', 'D', 'Sze', 'Journal', 'of', 'Geophysical', 'Research', '95', '18583', '18590', '1990', 'Charles', 'Jackman', 'Atmospheric', 'Chemistry', 'and', 'Dynamics', 'Branch', 'Code', '916', 'NASA', 'Goddard', 'Space', 'Flight', 'Center', 'Greenbelt', 'MD', '20771', 'Also', 'see', '_Chemical', 'Rockets', 'and', 'the', 'Environment_', 'A', 'McDonald', 'R', 'Bennett', 'J', 'Hinshaw', 'and', 'M', 'Barnes', 'Aerospace', 'America', 'May', '1991', 'HOW', 'LONG', 'CAN', 'A', 'HUMAN', 'LIVE', 'UNPROTECTED', 'IN', 'SPACE', 'If', 'you', 'don', 't', 'try', 'to', 'hold', 'your', 'breath', 'exposure', 'to', 'space', 'for', 'half', 'a', 'minute', 'or', 'so', 'is', 'unlikely', 'to', 'produce', 'permanent', 'injury', 'Holding', 'your', 'breath', 'is', 'likely', 'to', 'damage', 'your', 'lungs', 'something', 'scuba', 'divers', 'have', 'to', 'watch', 'out', 'for', 'when', 'ascending', 'and', 'you', 'll', 'have', 'eardrum', 'trouble', 'if', 'your', 'Eustachian', 'tubes', 'are', 'badly', 'plugged', 'up', 'but', 'theory', 'predicts', 'and', 'animal', 'experiments', 'confirm', 'that', 'otherwise', 'exposure', 'to', 'vacuum', 'causes', 'no', 'immediate', 'injury', 'You', 'do', 'not', 'explode', 'Your', 'blood', 'does', 'not', 'boil', 'You', 'do', 'not', 'freeze', 'You', 'do', 'not', 'instantly', 'lose', 'consciousness', 'Various', 'minor', 'problems', 'sunburn', 'possibly', 'the', 'bends', 'certainly', 'some', 'mild', 'reversible', 'painless', 'swelling', 'of', 'skin', 'and', 'underlying', 'tissue', 'start', 'after', 'ten', 'seconds', 'or', 'so', 'At', 'some', 'point', 'you', 'lose', 'consciousness', 'from', 'lack', 'of', 'oxygen', 'Injuries', 'accumulate', 'After', 'perhaps', 'one', 'or', 'two', 'minutes', 'you', 're', 'dying', 'The', 'limits', 'are', 'not', 'really', 'known', 'References', '_The', 'Effect', 'on', 'the', 'Chimpanzee', 'of', 'Rapid', 'Decompression', 'to', 'a', 'Near', 'Vacuum_', 'Alfred', 'G', 'Koestler', 'ed', 'NASA', 'CR', '329', 'Nov', '1965', '_Experimental', 'Animal', 'Decompression', 'to', 'a', 'Near', 'Vacuum', 'Environment_', 'R', 'W', 'Bancroft', 'J', 'E', 'Dunn', 'eds', 'Report', 'SAM', 'TR', '65', '48', 'June', '1965', 'USAF', 'School', 'of', 'Aerospace', 'Medicine', 'Brooks', 'AFB', 'Texas', 'HOW', 'THE', 'CHALLENGER', 'ASTRONAUTS', 'DIED', 'The', 'Challenger', 'shuttle', 'launch', 'was', 'not', 'destroyed', 'in', 'an', 'explosion', 'This', 'is', 'a', 'well', 'documented', 'fact', 'see', 'the', 'Rogers', 'Commission', 'report', 'for', 'example', 'What', 'looked', 'like', 'an', 'explosion', 'was', 'fuel', 'burning', 'after', 'the', 'external', 'tank', 'came', 'apart', 'The', 'forces', 'on', 'the', 'crew', 'cabin', 'were', 'not', 'sufficient', 'to', 'kill', 'the', 'astronauts', 'never', 'mind', 'destroy', 'their', 'bodies', 'according', 'to', 'the', 'Kerwin', 'team', 's', 'medical', 'forensic', 'report', 'The', 'astronauts', 'were', 'killed', 'when', 'the', 'more', 'or', 'less', 'intact', 'cabin', 'hit', 'the', 'water', 'at', 'circa', '200MPH', 'and', 'their', 'bodies', 'then', 'spent', 'several', 'weeks', 'underwater', 'Their', 'remains', 'were', 'recovered', 'and', 'after', 'the', 'Kerwin', 'team', 'examined', 'them', 'they', 'were', 'sent', 'off', 'to', 'be', 'buried', 'USING', 'THE', 'SHUTTLE', 'BEYOND', 'LOW', 'EARTH', 'ORBIT', 'You', 'can', 't', 'use', 'the', 'shuttle', 'orbiter', 'for', 'missions', 'beyond', 'low', 'Earth', 'orbit', 'because', 'it', 'can', 't', 'get', 'there', 'It', 'is', 'big', 'and', 'heavy', 'and', 'does', 'not', 'carry', 'enough', 'fuel', 'even', 'if', 'you', 'fill', 'part', 'of', 'the', 'cargo', 'bay', 'with', 'tanks', 'Furthermore', 'it', 'is', 'not', 'particularly', 'sensible', 'to', 'do', 'so', 'because', 'much', 'of', 'that', 'weight', 'is', 'things', 'like', 'wings', 'which', 'are', 'totally', 'useless', 'except', 'in', 'the', 'immediate', 'vicinity', 'of', 'the', 'Earth', 'The', 'shuttle', 'orbiter', 'is', 'highly', 'specialized', 'for', 'travel', 'between', 'Earth', 's', 'surface', 'and', 'low', 'orbit', 'Taking', 'it', 'higher', 'is', 'enormously', 'costly', 'and', 'wasteful', 'A', 'much', 'better', 'approach', 'would', 'be', 'to', 'use', 'shuttle', 'subsystems', 'to', 'build', 'a', 'specialized', 'high', 'orbit', 'spacecraft', 'Yet', 'another', 'concise', 'answer', 'by', 'Henry', 'Spencer', 'THE', 'FACE', 'ON', 'MARS', 'There', 'really', 'is', 'a', 'big', 'rock', 'on', 'Mars', 'that', 'looks', 'remarkably', 'like', 'a', 'humanoid', 'face', 'It', 'appears', 'in', 'two', 'different', 'frames', 'of', 'Viking', 'Orbiter', 'imagery', '35A72', 'much', 'more', 'facelike', 'in', 'appearance', 'and', 'the', 'one', 'more', 'often', 'published', 'with', 'the', 'Sun', '10', 'degrees', 'above', 'western', 'horizon', 'and', '70A13', 'with', 'the', 'Sun', '27', 'degrees', 'from', 'the', 'west', 'Science', 'writer', 'Richard', 'Hoagland', 'has', 'championed', 'the', 'idea', 'that', 'the', 'Face', 'is', 'artificial', 'intended', 'to', 'resemble', 'a', 'human', 'and', 'erected', 'by', 'an', 'extraterrestrial', 'civilization', 'Most', 'other', 'analysts', 'concede', 'that', 'the', 'resemblance', 'is', 'most', 'likely', 'accidental', 'Other', 'Viking', 'images', 'show', 'a', 'smiley', 'faced', 'crater', 'and', 'a', 'lava', 'flow', 'resembling', 'Kermit', 'the', 'Frog', 'elsewhere', 'on', 'Mars', 'There', 'exists', 'a', 'Mars', 'Anomalies', 'Research', 'Society', 'sorry', 'don', 't', 'know', 'the', 'address', 'to', 'study', 'the', 'Face', 'The', 'Mars', 'Observer', 'mission', 'will', 'carry', 'an', 'extremely', 'high', 'resolution', 'camera', 'and', 'better', 'images', 'of', 'the', 'formation', 'will', 'hopefully', 'settle', 'this', 'question', 'in', 'a', 'few', 'years', 'In', 'the', 'meantime', 'speculation', 'about', 'the', 'Face', 'is', 'best', 'carried', 'on', 'in', 'the', 'altnet', 'group', 'alt', 'alien', 'visitors', 'not', 'sci', 'space', 'or', 'sci', 'astro', 'V', 'DiPeitro', 'and', 'G', 'Molenaar', 'Unusual', 'Martian', 'Surface', 'Features', 'Mars', 'Research', 'P', 'O', 'Box', '284', 'Glen', 'Dale', 'Maryland', 'USA', '1982', '18', 'by', 'mail', 'R', 'R', 'Pozos', 'The', 'Face', 'of', 'Mars', 'Chicago', 'Review', 'Press', '1986', 'Account', 'of', 'an', 'interdisciplinary', 'speculative', 'conference', 'Hoagland', 'organized', 'to', 'investigate', 'the', 'Face', 'R', 'C', 'Hoagland', 'The', 'Monuments', 'of', 'Mars', 'A', 'City', 'on', 'the', 'Edge', 'of', 'Forever', 'North', 'Atlantic', 'Books', 'Berkeley', 'California', 'USA', '1987', 'Elaborate', 'discussion', 'of', 'evidence', 'and', 'speculation', 'that', 'formations', 'near', 'the', 'Face', 'form', 'a', 'city', 'M', 'J', 'Carlotto', 'Digital', 'Imagery', 'Analysis', 'of', 'Unusual', 'Martian', 'Surface', 'Features', 'Applied', 'Optics', '27', 'pp', '1926', '1933', '1987', 'Extracts', 'three', 'dimensional', 'model', 'for', 'the', 'Face', 'from', 'the', '2', 'D', 'images', 'M', 'J', 'Carlotto', 'M', 'C', 'Stein', 'A', 'Method', 'of', 'Searching', 'for', 'Artificial', 'Objects', 'on', 'Planetary', 'Surfaces', 'Journal', 'of', 'the', 'British', 'Interplanetary', 'Society', 'Vol', '43', 'no', '5', 'May', '1990', 'p', '209', '216', 'Uses', 'a', 'fractal', 'image', 'analysis', 'model', 'to', 'guess', 'whether', 'the', 'Face', 'is', 'artificial', 'B', 'O', 'Leary', 'Analysis', 'of', 'Images', 'of', 'the', 'Face', 'on', 'Mars', 'and', 'Possible', 'Intelligent', 'Origin', 'JBIS', 'Vol', '43', 'no', '5', 'May', '1990', 'p', '203', '208', 'Lights', 'Carlotto', 's', 'model', 'from', 'the', 'two', 'angles', 'and', 'shows', 'it', 's', 'consistent', 'shows', 'that', 'the', 'Face', 'doesn', 't', 'look', 'facelike', 'if', 'observed', 'from', 'the', 'surface', 'NEXT', 'FAQ', '13', '15', 'Space', 'activist', 'interest', 'research', 'groups', 'space', 'publications']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+', gaps=False)\n",
    "tokenized_texts = texts.map(\n",
    "    lambda path_text: tokenizer.tokenize(path_text[1]))\n",
    "print('\\n\\ntokenized text of first document')\n",
    "print(tokenized_texts.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "yrlw1Gib3Coz",
    "outputId": "b9aab2d0-1601-4fbb-c2cd-4737b86817e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "document-level counts of first document\n",
      "[('the', 92), ('and', 47), ('of', 44), ('to', 40), ('The', 25), ('a', 25), ('in', 24), ('is', 24), ('on', 19), ('for', 15)]\n"
     ]
    }
   ],
   "source": [
    "# This is the equivalent place that the previous implementations \n",
    "# started\n",
    "document_token_counts = tokenized_texts.map(Counter)\n",
    "print('\\n\\ndocument-level counts of first document')\n",
    "print(document_token_counts.first().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "eZEbtGxk3D96",
    "outputId": "141e63c1-e557-4f1f-83e5-5e7a7e8c89c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "word counts\n",
      "[('the', 1648), ('of', 804), ('edu', 784), ('to', 770), ('and', 641), ('a', 615), ('in', 402), ('is', 352), ('for', 305), ('cmu', 288)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = token_counts = document_token_counts.reduce(add)\n",
    "print('\\n\\nword counts')\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cqJ7p9s3GPo"
   },
   "source": [
    "As you see, we use the map and reduce methods here. Spark allows you to implement MapReduce-style programs, but you can also implement in many other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Crtr94y3JTF"
   },
   "source": [
    "#### Python and R\n",
    "\n",
    "Spark is primarily implemented in Scala. The Java API is there to allow more idiomatic Java use of Spark.     There is also a Python API (PySpark) and an R API (SparkR). Spark-based programs implemented in Scala or Java run on the same JVM that serves as the driver. Programs implemented in PySpark or SparkR run in Python and R processes, respectively, with the SparkSession ultimately in a different process. This generally does not affect the performance, unless we use functions defined in Python or R.\n",
    "\n",
    "As can be seen in the previous example, when we are tokenizing, counting, and combining counts we are calling Python code to process our data. This is accomplished by the JVM process serializing and shipping the data to the Python process, which is then deserialized, processed, serialized, and shipped back to the JVM to be deserialized. This adds a lot of extra work to our job. When using PySpark or SparkR it will be faster to use internal Spark functions whenever possible.\n",
    "\n",
    "Not using custom functions in Python or R seems restrictive when using `RDD`s, but most likely, your work will be using the `DataFrame`s and `DataSet`s that we discuss in the next section  .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjGyqDiA3Pzd"
   },
   "source": [
    "## Spark SQL and Spark MLlib\n",
    "\n",
    "Since the release of Spark 2,  the primary intended way to work with data within Spark is through the `Dataset`. The `Dataset[T]` is an object that allows us to treat our distributed data as tables. The type parameter `T` is the type used to represent the rows of the table. There is a special kind of `Dataset` in which the type of the rows is `Row`, which allows us to have tabular data without defining new classes—this does come at the cost of losing some type safety. The examples we'll be using will generally be with `DataFrame`s, since they are the best way to work with data in PySpark.\n",
    "\n",
    "The `Dataset` and `DataFrame` are defined in the Spark SQL module, since one of the greatest benefits is the ability to express many operations with SQL.   The prebuilt user-defined functions (`UDF`s) are available in all the APIs. This allows us to do most kinds of processing in the non-JVM languages with the same efficiency as if we were using Scala or Java.\n",
    "\n",
    "Another module we need to introduce before we begin talking about Spark NLP is MLlib. MLlib is a module for doing machine learning on Spark. Before Spark 2, all the MLlib algorithms were implemented on `RDD`s. Since then, a new version of MLlib was defined using `Dataset`s and `DataFrame`s. MLlib is similar in design, at a high level, to other machine learning libraries, with a notion of transformers, models, and pipelines.\n",
    "\n",
    "Before we talk about MLlib, let's load some data into a `DataFrame`, since MLlib is built using `DataFrame`s.  We will be using the Iris data set, which is often used as an example in data science. It's small, easy to understand, and can work for clustering and classification examples. It is structured data, so it doesn't give us any text data to work with. Table-like structure is generally designed around structured data, so this data will help us explore the API before getting into using Spark for text.\n",
    "\n",
    "The `iris.data` file does not have a header, so we have to tell Spark what the columns are when they are loaded. Let's construct a schema.  The schema is the definition of the columns and their types in the `DataFrame`. The most common task is to build a model to predict what class an iris flower is (I. virginica, I. setosa, or I. versicolor) based on its sepals and petals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iarB5A523E_5"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od1TGFPv36cS"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('sepal_length', DoubleType(), nullable=False),\n",
    "    StructField('sepal_width', DoubleType(), nullable=False),\n",
    "    StructField('petal_length', DoubleType(), nullable=False),\n",
    "    StructField('petal_width', DoubleType(), nullable=False),\n",
    "    StructField('class', StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXkOH7Fv3_oJ"
   },
   "outputs": [],
   "source": [
    "iris = spark.read.csv('./data/iris.data', schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "k8mb_RIV4gG5",
    "outputId": "001fbfff-4432-44f3-e69c-fdc0e8186d91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.843333333333335</td>\n",
       "      <td>3.0540000000000007</td>\n",
       "      <td>3.7586666666666693</td>\n",
       "      <td>1.1986666666666672</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.8280661279778637</td>\n",
       "      <td>0.43359431136217375</td>\n",
       "      <td>1.764420419952262</td>\n",
       "      <td>0.7631607417008414</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        sepal_length          sepal_width        petal_length  \\\n",
       "0   count                 150                  150                 150   \n",
       "1    mean   5.843333333333335   3.0540000000000007  3.7586666666666693   \n",
       "2  stddev  0.8280661279778637  0.43359431136217375   1.764420419952262   \n",
       "3     min                 4.3                  2.0                 1.0   \n",
       "4     max                 7.9                  4.4                 6.9   \n",
       "\n",
       "          petal_width           class  \n",
       "0                 150             150  \n",
       "1  1.1986666666666672            None  \n",
       "2  0.7631607417008414            None  \n",
       "3                 0.1     Iris-setosa  \n",
       "4                 2.5  Iris-virginica  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F9XRpbd4p9w"
   },
   "source": [
    "Let's start by looking at some of the summary stats for the Iris setosa class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "Xy9s9xWP4k9R",
    "outputId": "b2cfda21-4008-4d3d-9691-1e1686b79019"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class\n",
       "0   Iris-virginica\n",
       "1      Iris-setosa\n",
       "2  Iris-versicolor"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.select('class').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbsRnoJe4qiv"
   },
   "source": [
    "We can register a `DataFrame`, which will allow us to interact with it purely through SQL. We will be registering our `DataFrame` as a temporary table. This means that the table will exist only for the lifetime of our `App`, and it will be available only through our `App`'s `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQeIiXFk4nov"
   },
   "outputs": [],
   "source": [
    "iris.registerTempTable('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2Nr8T9Lb42Ng",
    "outputId": "dc6cfc0d-5f74-411b-94a0-2814722b80dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT *\n",
    "FROM iris\n",
    "LIMIT 5\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niPjvvGV45Qe"
   },
   "source": [
    "Let's look at some of the fields grouped by their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Tc847MrT43HQ",
    "outputId": "837314c2-b010-447e-c0ef-24bdfc37caa5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>min(sepal_length)</th>\n",
       "      <th>avg(sepal_length)</th>\n",
       "      <th>max(sepal_length)</th>\n",
       "      <th>min(sepal_width)</th>\n",
       "      <th>avg(sepal_width)</th>\n",
       "      <th>max(sepal_width)</th>\n",
       "      <th>min(petal_length)</th>\n",
       "      <th>avg(petal_length)</th>\n",
       "      <th>max(petal_length)</th>\n",
       "      <th>min(petal_width)</th>\n",
       "      <th>avg(petal_width)</th>\n",
       "      <th>max(petal_width)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.588</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.974</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.552</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.026</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.006</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.418</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.936</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.260</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  min(sepal_length)  avg(sepal_length)  max(sepal_length)  \\\n",
       "0   Iris-virginica                4.9              6.588                7.9   \n",
       "1      Iris-setosa                4.3              5.006                5.8   \n",
       "2  Iris-versicolor                4.9              5.936                7.0   \n",
       "\n",
       "   min(sepal_width)  avg(sepal_width)  max(sepal_width)  min(petal_length)  \\\n",
       "0               2.2             2.974               3.8                4.5   \n",
       "1               2.3             3.418               4.4                1.0   \n",
       "2               2.0             2.770               3.4                3.0   \n",
       "\n",
       "   avg(petal_length)  max(petal_length)  min(petal_width)  avg(petal_width)  \\\n",
       "0              5.552                6.9               1.4             2.026   \n",
       "1              1.464                1.9               0.1             0.244   \n",
       "2              4.260                5.1               1.0             1.326   \n",
       "\n",
       "   max(petal_width)  \n",
       "0               2.5  \n",
       "1               0.6  \n",
       "2               1.8  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "    class, \n",
    "    min(sepal_length), avg(sepal_length), max(sepal_length),\n",
    "    min(sepal_width), avg(sepal_width), max(sepal_width),\n",
    "    min(petal_length), avg(petal_length), max(petal_length),\n",
    "    min(petal_width), avg(petal_width), max(petal_width)\n",
    "FROM iris\n",
    "GROUP BY class\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMOw8zNL48P3"
   },
   "source": [
    "### Transformers\n",
    "\n",
    "A `Transformer` is a piece of logic that transforms the data without needing to learn or fit anything from the data. A good way to understand transformers is that they represent functions that we wish to map over our data. All stages of a pipeline have parameters so that we can make sure that the transformation is being applied to the right fields and with the desired configuration. Let's look at a few examples.\n",
    "\n",
    "#### SQLTransformer\n",
    "\n",
    "The `SQLTransformer` has only one parameter—`statement`—which is the SQL statement that will be executed against our `DataFrame`. Let's use an `SQLTransformer` to do the group-by we performed previously. table0306 shows the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXQnJvqm46sY"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "statement = '''\n",
    "SELECT \n",
    "    class, \n",
    "    min(sepal_length), avg(sepal_length), max(sepal_length),\n",
    "    min(sepal_width), avg(sepal_width), max(sepal_width),\n",
    "    min(petal_length), avg(petal_length), max(petal_length),\n",
    "    min(petal_width), avg(petal_width), max(petal_width)\n",
    "FROM iris\n",
    "GROUP BY class\n",
    "'''\n",
    "\n",
    "sql_transformer = SQLTransformer(statement=statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "nnzBVd1w5Xw4",
    "outputId": "cd6543ec-ffea-4551-ed01-86b0978f2605"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>min(sepal_length)</th>\n",
       "      <th>avg(sepal_length)</th>\n",
       "      <th>max(sepal_length)</th>\n",
       "      <th>min(sepal_width)</th>\n",
       "      <th>avg(sepal_width)</th>\n",
       "      <th>max(sepal_width)</th>\n",
       "      <th>min(petal_length)</th>\n",
       "      <th>avg(petal_length)</th>\n",
       "      <th>max(petal_length)</th>\n",
       "      <th>min(petal_width)</th>\n",
       "      <th>avg(petal_width)</th>\n",
       "      <th>max(petal_width)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.588</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.974</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.552</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.026</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.006</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.418</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.936</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.260</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  min(sepal_length)  avg(sepal_length)  max(sepal_length)  \\\n",
       "0   Iris-virginica                4.9              6.588                7.9   \n",
       "1      Iris-setosa                4.3              5.006                5.8   \n",
       "2  Iris-versicolor                4.9              5.936                7.0   \n",
       "\n",
       "   min(sepal_width)  avg(sepal_width)  max(sepal_width)  min(petal_length)  \\\n",
       "0               2.2             2.974               3.8                4.5   \n",
       "1               2.3             3.418               4.4                1.0   \n",
       "2               2.0             2.770               3.4                3.0   \n",
       "\n",
       "   avg(petal_length)  max(petal_length)  min(petal_width)  avg(petal_width)  \\\n",
       "0              5.552                6.9               1.4             2.026   \n",
       "1              1.464                1.9               0.1             0.244   \n",
       "2              4.260                5.1               1.0             1.326   \n",
       "\n",
       "   max(petal_width)  \n",
       "0               2.5  \n",
       "1               0.6  \n",
       "2               1.8  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_transformer.transform(iris).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg8MfQqg5dyc"
   },
   "source": [
    "We get the same output as when we ran the SQL command.\n",
    "\n",
    "SQLTransformer is useful when you have preprocessing or restructuring that you need to perform on your data before other steps in the pipeline. Now let's look at a transformer that works on one field and returns the original data with a new field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0u134bux5fkc"
   },
   "source": [
    "#### Binarizer\n",
    "\n",
    "The `Binarizer` is a `Transformer` that applies a threshold to a numeric field, turning it into 0s (when below the threshold) and 1s (when above the threshold). It takes three parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be binarized\n",
    "* outputCol  \n",
    "The column containing the binarized values\n",
    "* threshold  \n",
    "The threshold we will apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khg_A1gK5cbS"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(\n",
    "    inputCol='sepal_length', \n",
    "    outputCol='sepal_length_above_5', \n",
    "    threshold=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "BBsbfx395v0A",
    "outputId": "d34c1e88-e0f5-4c84-e761-eb09558bcf14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>sepal_length_above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "   sepal_length_above_5  \n",
       "0                   1.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.transform(iris).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPilfKc85xsG"
   },
   "source": [
    "Unlike the `SQLTransformer`, the `Binarizer` returns a modified version of the input `DataFrame`. Almost all Transformers behave this way.\n",
    "\n",
    "The `Binarizer` is used when you want to convert a real valued property into a class. For example, if we want to mark social media posts as \"viral\" and \"not-viral\" we could use a `Binarizer` on the views property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXAFplFK53sH"
   },
   "source": [
    "#### VectorAssembler\n",
    "\n",
    "Another import Transformer is the VectorAssembler. It takes a list of numeric and vector-valued columns and constructs a single vector. This is useful because all MLlib's machine learning algorithms expect a single vector-valued input column for features. The VectorAssembler takes two parameters:\n",
    "\n",
    "* inputCols  \n",
    "The list of columns to be assembled\n",
    "* outputCol  \n",
    "The column containing the new vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g70IjXs558yQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'sepal_length', 'sepal_width', \n",
    "        'petal_length', 'petal_width'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTTAhTU35-rJ"
   },
   "source": [
    "Let's persist this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4EPffJx5-QO"
   },
   "outputs": [],
   "source": [
    "iris_w_vecs = assembler.transform(iris).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "KqWV8YQF6A1A",
    "outputId": "01e4aadf-5dc0-4152-c7f2-8584660a65f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  \n",
       "0  [5.1, 3.5, 1.4, 0.2]  \n",
       "1  [4.9, 3.0, 1.4, 0.2]  \n",
       "2  [4.7, 3.2, 1.3, 0.2]  \n",
       "3  [4.6, 3.1, 1.5, 0.2]  \n",
       "4  [5.0, 3.6, 1.4, 0.2]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_vecs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOAHYDh96DVb"
   },
   "source": [
    "### Estimators and Models\n",
    "\n",
    "`Estimator`s allow us to create transformations that are informed by our data. Classification models (e.g., decision trees) and regression models (e.g., linear regressions) are prominent examples, but some preprocessing algorithms are like this as well. For example, preprocessing that needs to know the whole vocabulary first will be `Estimator`s. The `Estimator` is fit with a `DataFrame` and returns a `Model`, which is a kind of Transformer. The `Model`s created from classifier and regression `Estimator`s are `PredictionModel`s.\n",
    "\n",
    "This is a similar design to scikit-learn, with the exception that in scikit-learn when we call `fit` we mutate the estimator instead of creating a new object. There are pros and cons to this, as there always are when debating mutability. Idiomatic Scala strongly prefers immutability.\n",
    "\n",
    "Let's look at some examples of `Estimator`s and `Model`s.\n",
    "\n",
    "#### MinMaxScaler\n",
    "\n",
    "The MinMaxScaler allows us to scale our data to be between 0 and 1. It takes four parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be scaled\n",
    "* outputCol  \n",
    "The column containing the scaled values\n",
    "* max  \n",
    "The new maximum value (optional, default = 1)\n",
    "* min  \n",
    "The new minimum value (optional, default = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dc5dBhtZ6B4w"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='features', \n",
    "    outputCol='petal_length_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wepcwUUX6h0v"
   },
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(iris_w_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "YqiwVXvr6iu_",
    "outputId": "ccfbd608-f1df-497c-df50-74d07dc44356"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>petal_length_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>[0.22222222222222213, 0.6249999999999999, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>[0.1666666666666668, 0.41666666666666663, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>[0.11111111111111119, 0.5, 0.05084745762711865...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>[0.08333333333333327, 0.4583333333333333, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>[0.19444444444444448, 0.6666666666666666, 0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features                                petal_length_scaled  \n",
       "0  [5.1, 3.5, 1.4, 0.2]  [0.22222222222222213, 0.6249999999999999, 0.06...  \n",
       "1  [4.9, 3.0, 1.4, 0.2]  [0.1666666666666668, 0.41666666666666663, 0.06...  \n",
       "2  [4.7, 3.2, 1.3, 0.2]  [0.11111111111111119, 0.5, 0.05084745762711865...  \n",
       "3  [4.6, 3.1, 1.5, 0.2]  [0.08333333333333327, 0.4583333333333333, 0.08...  \n",
       "4  [5.0, 3.6, 1.4, 0.2]  [0.19444444444444448, 0.6666666666666666, 0.06...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model.transform(iris_w_vecs).limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCn3_GG76lmz"
   },
   "source": [
    "Notice that the `petal_length_scaled` column now has values between 0 and 1. This can help some training algorithms, specifically those that have difficulty combining features of different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsGomFZx6n-P"
   },
   "source": [
    "#### StringIndexer\n",
    "\n",
    "Let's build a model! We will try and predict the class from the other features, and we will use a decision tree. First, though, we must convert our target into index values.\n",
    "\n",
    "The `StringIndexer` `Estimator` will turn our class values into indices. We want to do this to simplify some of the downstream processing. It is simpler to implement most training algorithms with the assumption that the target is a number. The `StringIndexer` takes four parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be indexed\n",
    "* outputCol  \n",
    "The column containing the indexed values\n",
    "* handleInvalid  \n",
    "The policy for how the model should handle values not seen by the estimator (optional, default = error)\n",
    "* stringOrderType  \n",
    "How to order the values to make the indexing deterministic (optional, default = frequencyDesc)\n",
    "\n",
    "We will also want an `IndexToString` `Transformer`. This will let us map our predictions, which will be indices, back to string values. IndexToString takes three parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be mapped\n",
    "* outputCol  \n",
    "The column containing the mapped values\n",
    "* labels  \n",
    "The mapping from index to value, usually generated by StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXK1Egxc6kAO"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "\n",
    "indexer = StringIndexer(inputCol='class', outputCol='class_ix')\n",
    "indexer_model = indexer.fit(iris_w_vecs)\n",
    "\n",
    "index2string = IndexToString(\n",
    "    inputCol=indexer_model.getOrDefault('outputCol'), \n",
    "    outputCol='pred_class', \n",
    "    labels=indexer_model.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNd2mh-S679n"
   },
   "outputs": [],
   "source": [
    "iris_indexed = indexer_model.transform(iris_w_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3Fh42X36-O7"
   },
   "source": [
    "Now we are ready to train our `DecisionTreeClassifier`. This `Estimator` has many parameters, so I recommend you become familiar with the APIs. They are all well documented in the [PySpark API documentation](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQL3f3oc6873"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt_clfr = DecisionTreeClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='class_ix',\n",
    "    maxDepth=5,\n",
    "    impurity='gini',\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rmJmMg77OPX"
   },
   "outputs": [],
   "source": [
    "dt_clfr_model = dt_clfr.fit(iris_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oVTGw9E7PGn"
   },
   "outputs": [],
   "source": [
    "iris_w_pred = dt_clfr_model.transform(iris_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "Lx6b899H7RoH",
    "outputId": "aad3013a-9879-4d1e-9cef-1f58d552e44d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_pred.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2zQHR8i7UB-"
   },
   "source": [
    "Now we need to map the predicted classes back to their string form using our `IndexToString`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqnw--uk7S0P"
   },
   "outputs": [],
   "source": [
    "iris_w_pred_class = index2string.transform(iris_w_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2l40t2Vl7X31",
    "outputId": "5f3fa9da-b01e-48ca-e5e5-60ce9868fabe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction   pred_class  \n",
       "0         0.0  Iris-setosa  \n",
       "1         0.0  Iris-setosa  \n",
       "2         0.0  Iris-setosa  \n",
       "3         0.0  Iris-setosa  \n",
       "4         0.0  Iris-setosa  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_pred_class.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9M6aBVpm7Z-N"
   },
   "source": [
    "How well did our model fit the data? Let's see how many predictions match the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_3ruUra7d5y"
   },
   "source": [
    "### Evaluators\n",
    "\n",
    "The evaluation options in MLlib are still limited compared to libraries like scikit-learn, but they can be useful if you are looking to create an easy-to-run training pipeline that calculates metrics.\n",
    "\n",
    "In our example, we are trying to solve a multiclass prediction problem, so we will use the `MulticlassClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDefCgpo7Y2e"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='class_ix', \n",
    "    metricName='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N-1SaVgK7jmW",
    "outputId": "2f2b03b8-6655-4bcb-bf2b-10897a41c688"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(iris_w_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFE0inS57mxw"
   },
   "source": [
    "This seems too good. What if we are overfit? Perhaps we should try using cross-validation to evaluate our models. Before we do that, let's organize stages into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ng0U11uo7qZy"
   },
   "source": [
    "### Pipelines\n",
    "\n",
    "Pipelines are a special kind of Estimator that takes a list of Transformers and Estimators and allows us to use them as a single Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTlhmX0Y7lS3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[assembler, indexer, dt_clfr, index2string]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CYSiXjY7u12"
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "SlQuCNcG7wGe",
    "outputId": "b64ca260-630d-4ffc-8758-38898628bd7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction   pred_class  \n",
       "0         0.0  Iris-setosa  \n",
       "1         0.0  Iris-setosa  \n",
       "2         0.0  Iris-setosa  \n",
       "3         0.0  Iris-setosa  \n",
       "4         0.0  Iris-setosa  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.transform(iris).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_f-wAMr7yoI"
   },
   "source": [
    "#### Cross validation\n",
    "Now that we have a `Pipeline` and an `Evaluator` we can create a `CrossValidator`. The `CrossValidator` itself is also an `Estimator`. When we call fit, it will fit our `pipeline` to each fold of data, and calculate the metric determined by our `Evaluator`. `CrossValidator` takes five parameters:\n",
    "\n",
    "* estimator  \n",
    "The `Estimator` to be tuned\n",
    "* estimatorParamMaps  \n",
    "The hyperparameter values to try in a hyperparameter grid search\n",
    "* evaluator  \n",
    "The `Evaluator` that calculates the metric\n",
    "* numFolds  \n",
    "The number of folds to split the data into\n",
    "* seed  \n",
    "A seed for making the splits reproducible\n",
    "\n",
    "We will make a trivial hyperparameter grid here, since we are only interested in estimating how well our model does on data it has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdXmuDHj7w-P"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt_clfr.maxDepth, [5]).\\\n",
    "    build()\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator, \n",
    "    numFolds=3, \n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SHltwkM8CqZ"
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUZs-fHz8EvK"
   },
   "source": [
    "Now, we can see how the model does when trained on two-thirds and evaluated on one-third. The `avgMetrics` in `cv_model` contains the average value of the designated metric across folds for each point in the hyperparameter grid tested. In our case, there is only one point in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E8bipoJ78Dyu",
    "outputId": "6d363955-e70f-4374-c758-bf84493c5545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9588996659642801]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTDKEZ3D8Jve"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4g-ER3_8NUw"
   },
   "source": [
    "Keep in mind that 95% accuracy is much more believable than 100%.\n",
    "\n",
    "There are many other `Transformer`s, `Estimator`s, and `Model`s. We will look into more as we continue, but for now, there is one more thing we need to discuss—saving our pipelines. \n",
    "\n",
    "#### Serialization of models\n",
    "\n",
    "MLlib allows us to save `Pipelines` so that we can use them later. We can also save individual `Transformers` and `Models`, but we will often want to keep all the stages of a `Pipeline` together. Generally speaking, we use separate programs for building models and using models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3KIXD5g8IPw"
   },
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('pipeline.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "L-RrvPhE8aj1",
    "outputId": "db996e65-d805-4622-d153-b379e0353b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.model/metadata:\r\n",
      "_SUCCESS  part-00000\r\n",
      "\r\n",
      "pipeline.model/stages:\r\n",
      "0_VectorAssembler_8913262eecb6\t2_DecisionTreeClassifier_5d2f444d4dc2\r\n",
      "1_StringIndexer_62cc1c2d10ca\t3_IndexToString_131c8d6c62c7\r\n"
     ]
    }
   ],
   "source": [
    "! ls pipeline.model/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDL8Agwr8dbb"
   },
   "source": [
    "## NLP Libraries\n",
    "\n",
    "There are two kinds of NLP libraries, generally speaking: functionality libraries and annotation libraries.\n",
    "\n",
    "### Functionality Libraries\n",
    "\n",
    "A functionality library is a collection of functions built for specific NLP tasks and techniques. Often, the functions are built without assuming that other functions will be used first. This means that functions like part-of-speech (POS) tagging will also perform tokenization. These libraries are good for research because it is often much easier to implement novel functions. On the other hand, because there is no unifying design, the performance of these libraries is generally much worse than that of annotation libraries.\n",
    "\n",
    "The Natural Language Tool Kit (NLTK) is a great functionality library.  It was originally created by Edward Loper. The landmark NLP book Natural Language Processing with Python (O'Reilly) was written by Steven Bird, Ewan Klein, and Edward Loper. I strongly recommend that book to anyone learning NLP. There a many useful and interesting modules in NLTK. It is, and will likely remain, the best NLP library for teaching NLP. The functions are not necessarily implemented with runtime performance or other productionization concerns in mind. If you are working on a research project and using a data set manageable on a single machine, you should consider NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCQSbWMt8knB"
   },
   "source": [
    "### Annotation Libraries\n",
    "   Annotation libraries are libraries in which all the functionality is built around a document-annotation model. There are three objects to keep in mind with annotation libraries: document, annotation, and annotator. The idea behind annotation libraries is to augment the incoming data with the results of our NLP functions.\n",
    "\n",
    "* Document  \n",
    "The document is the representation of the piece of text we wish to process. Naturally, the document must contain the text. Additionally, we often want to have an identifier associated with each document so that we can store our augmented data as structured data. This identifier will often be a title if the texts we are processing have titles.\n",
    "* Annotation  \n",
    "The annotation is the representation of the output of our NLP functions. For the annotation we need to have a type so that later processing knows how to interpret the annotations. Annotations also need to store their location within the document. For example, let's say the word \"pacing\" occurs 134 characters into the document. It will have 134 as the start, and 140 as the end. The lemma annotation for \"pacing\" will have the location. Some annotation libraries also have a concept of document-level annotation that does not have a location. There will be additional fields, depending on the type. Simple annotations like tokens generally don't have extra fields. Stem annotations usually have the stem that was extracted for the range of the text.\n",
    "* Annotator  \n",
    "The annotator is the object that contains the logic for using the NLP function. The annotator will often require configuration or external data sets. Additionally, there are model-based annotators. One of the benefits of an annotation library is that annotators can take advantage of the work done by previous annotators. This naturally creates a notion of a pipeline of annotators.\n",
    "\n",
    "#### spaCy\n",
    "\n",
    "spaCy is an \"industrial strength\" NLP library. I will give a brief description, but I encourage you to go and read their fantastic documentation. spaCy combines the document model just described with a model for the language being processed (English, Spanish, etc.), which has allowed spaCy to support multiple languages in a way that is easy for developers to use. Much of its functionality is implemented in Python to get the speed of native code. If you are working in an environment that is using only Python, and you are unlikely to run distributed processes, then spaCy is a great choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMV8HhcT8zcD"
   },
   "source": [
    "### NLP in Other Libraries\n",
    "\n",
    "There are some non-NLP libraries that have some NLP functionality. It is often in machine learning libraries to support machine learning on text data.\n",
    "\n",
    "* scikit-learn  \n",
    "A Python machine learning library that has functionality for extracting features from text. This functionality is generally of the bag-of-words kind of processing. The way these processes are built allows them to easily take advantage of more NLP-focused libraries.\n",
    "* Lucene  \n",
    "A Java document search framework that has some text-processing functionality necessary for building a search engine. We will use Lucene later on when we talk about information retrieval.\n",
    "* Gensim  \n",
    "A topic-modeling library (and it performs other distributional semantics techniques). Like spaCy, it is partially implemented in Cython, and like scikit-learn, it allows plug-and-play text processing in its API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5-K0vny88Ex"
   },
   "source": [
    "## Spark NLP\n",
    "\n",
    "The Spark NLP library was originally designed in early 2017 as an annotation library native to Spark to take full advantage of Spark SQL and MLlib modules. The inspiration came from trying to use Spark to distribute other NLP libraries, which were generally not implemented with concurrency or distributed computing in mind.\n",
    "\n",
    "### Annotation Library\n",
    " Spark NLP has the same concepts as any other annotation library but differs in how it stores annotations. Most annotation libraries store the annotations in the document object, but Spark NLP creates columns for the different types of annotations.\n",
    "\n",
    "The annotators are implemented as `Transformers`, `Estimators`, and `Models`. Let's take a look at some examples.\n",
    "\n",
    "### Stages\n",
    "\n",
    "One of the design principles of Spark NLP is easy interoperability with the existing algorithms in MLlib. Because there is no notion of documents or annotations in MLlib there are transformers for turning text columns into documents and converting annotations into vanilla Spark SQL data types. The usual usage pattern is as follows:\n",
    "\n",
    "1. Load data with Spark SQL.\n",
    "2. Create document column.\n",
    "3. Process with Spark NLP.\n",
    "4. Convert annotations of interest into Spark SQL data types.\n",
    "5. Run additional MLlib stages.\n",
    "\n",
    "We have already looked at how to load data with Spark SQL and how to use MLlib stages in the standard Spark library, so we will look at the middle three stages now. First, we will look at the DocumentAssembler (stage 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPIftf4H9NYN"
   },
   "source": [
    "#### Transformers\n",
    "\n",
    "To explore these five stages  we will again use the mini_newsgroups data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yocWLnFr8bkc"
   },
   "outputs": [],
   "source": [
    "from sparknlp import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUiwFbpl9SHO"
   },
   "outputs": [],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "texts = spark.sparkContext.wholeTextFiles(path)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('path', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "])\n",
    "\n",
    "texts = spark.createDataFrame(texts, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "a7sR_06c9TSF",
    "outputId": "48a454d6-03f9-4e42-b4f4-c30a84b3b1aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...  \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...  \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....  \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPPQpNsV9Vpu"
   },
   "source": [
    "#### DocumentAssembler\n",
    "\n",
    "The DocumentAssembler takes five parameters (see table0314):\n",
    "\n",
    "* inputCol  \n",
    "The column containing the text of the document\n",
    "* outputCol  \n",
    "The name of the column containing the newly constructed document\n",
    "* idCol  \n",
    "The name of the column containing the identifier (optional)\n",
    "* metadataCol  \n",
    "The name of a Map-type column that represents document metadata (optional)\n",
    "* trimAndClearNewLines  \n",
    "Determines whether to remove new line characters and trim strings (optional, default = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ek4VTEG9UQF"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\\\n",
    "    .setIdCol('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaOFSlsT9etd"
   },
   "outputs": [],
   "source": [
    "docs = document_assembler.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "GsFEAbk19gLd",
    "outputId": "5fdfb6c0-0838-4bc6-801d-8e0ac39b9493"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....  \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...  \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...  \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...  \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "MF6edG1V9hP1",
    "outputId": "d31cb75f-f6eb-4833-c2af-e910bd669ae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotatorType': 'document',\n",
       " 'begin': 0,\n",
       " 'embeddings': [],\n",
       " 'end': 13095,\n",
       " 'metadata': {'id': 'file:/home/alex/projects/spark-nlp-book-prod/jupyter/data/mini_newsgroups/sci.space/59848',\n",
       "  'sentence': '0'},\n",
       " 'result': 'Xref: cantaloupe.srv.cs.cmu.edu sci.space:59848 sci.answers:100 news.answers:7198\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!ogicse!decwrl!concert!borg.cs.unc.edu!not-for-mail\\nFrom: leech@cs.unc.edu (Jon Leech)\\nNewsgroups: sci.space,sci.answers,news.answers\\nSubject: Space FAQ 12/15 - Controversial Questions\\nKeywords: Frequently Asked Questions\\nMessage-ID: <controversy_733694426@cs.unc.edu>\\nDate: 1 Apr 93 10:00:29 GMT\\nArticle-I.D.: cs.controversy_733694426\\nExpires: 6 May 1993 20:00:26 GMT\\nReferences: <diffs_733693816@cs.unc.edu>\\nFollowup-To: poster\\nDistribution: world\\nOrganization: University of North Carolina, Chapel Hill\\nLines: 252\\nApproved: news-answers-request@MIT.Edu\\nSupersedes: <controversy_730956589@cs.unc.edu>\\nNNTP-Posting-Host: mahler.cs.unc.edu\\n\\nArchive-name: space/controversy\\nLast-modified: $Date: 93/04/01 14:39:06 $\\n\\nCONTROVERSIAL QUESTIONS\\n\\n    These issues periodically come up with much argument and few facts being\\n    offered. The summaries below attempt to represent the position on which\\n    much of the net community has settled. Please DON\\'T bring them up again\\n    unless there\\'s something truly new to be discussed. The net can\\'t set\\n    public policy, that\\'s what your representatives are for.\\n\\n\\n    WHAT HAPPENED TO THE SATURN V PLANS\\n\\n    Despite a widespread belief to the contrary, the Saturn V blueprints\\n    have not been lost. They are kept at Marshall Space Flight Center on\\n    microfilm.\\n\\n    The problem in re-creating the Saturn V is not finding the drawings, it\\n    is finding vendors who can supply mid-1960\\'s vintage hardware (like\\n    guidance system components), and the fact that the launch pads and VAB\\n    have been converted to Space Shuttle use, so you have no place to launch\\n    from.\\n\\n    By the time you redesign to accommodate available hardware and re-modify\\n    the launch pads, you may as well have started from scratch with a clean\\n    sheet design.\\n\\n\\n    WHY DATA FROM SPACE MISSIONS ISN\\'T IMMEDIATELY AVAILABLE\\n\\n    Investigators associated with NASA missions are allowed exclusive access\\n    for one year after the data is obtained in order to give them an\\n    opportunity to analyze the data and publish results without being\\n    \"scooped\" by people uninvolved in the mission. However, NASA frequently\\n    releases examples (in non-digital form, e.g. photos) to the public early\\n    in a mission.\\n\\n\\n    RISKS OF NUCLEAR (RTG) POWER SOURCES FOR SPACE PROBES\\n\\n    There has been extensive discussion on this topic sparked by attempts to\\n    block the Galileo and Ulysses launches on grounds of the plutonium\\n    thermal sources being dangerous. Numerous studies claim that even in\\n    worst-case scenarios (shuttle explosion during launch, or accidental\\n    reentry at interplanetary velocities), the risks are extremely small.\\n    Two interesting data points are (1) The May 1968 loss of two SNAP 19B2\\n    RTGs, which landed intact in the Pacific Ocean after a Nimbus B weather\\n    satellite failed to reach orbit. The fuel was recovered after 5 months\\n    with no release of plutonium. (2) In April 1970, the Apollo 13 lunar\\n    module reentered the atmosphere and its SNAP 27 RTG heat source, which\\n    was jettisoned, fell intact into the 20,000 feet deep Tonga Trench in\\n    the Pacific Ocean. The corrosion resistant materials of the RTG are\\n    expected to prevent release of the fuel for a period of time equal to 10\\n    half-lives of the Pu-238 fuel or about 870 years [DOE 1980].\\n\\n    To make your own informed judgement, some references you may wish to\\n    pursue are:\\n\\n    A good review of the technical facts and issues is given by Daniel\\n    Salisbury in \"Radiation Risk and Planetary Exploration-- The RTG\\n    Controversy,\" *Planetary Report*, May-June 1987, pages 3-7. Another good\\n    article, which also reviews the events preceding Galileo\\'s launch,\\n    \"Showdown at Pad 39-B,\" by Robert G. Nichols, appeared in the November\\n    1989 issue of *Ad Astra*. (Both magazines are published by pro-space\\n    organizations, the Planetary Society and the National Space Society\\n    respectively.)\\n\\n    Gordon L Chipman, Jr., \"Advanced Space Nuclear Systems\" (AAS 82-261), in\\n    *Developing the Space Frontier*, edited by Albert Naumann and Grover\\n    Alexander, Univelt, 1983, p. 193-213.\\n\\n    \"Hazards from Plutonium Toxicity\", by Bernard L. Cohen, Health Physics,\\n    Vol 32 (may) 1977, page 359-379.\\n\\n    NUS Corporation, Safety Status Report for the Ulysses Mission: Risk\\n    Analysis (Book 1). Document number is NUS 5235; there is no GPO #;\\n    published Jan 31, 1990.\\n\\n    NASA Office of Space Science and Applications, *Final Environmental\\n    Impact Statement for the Ulysses Mission (Tier 2)*, (no serial number or\\n    GPO number, but probably available from NTIS or NASA) June 1990.\\n\\n    [DOE 1980] U.S.  Department of Energy, *Transuranic Elements in the\\n    Environment*, Wayne C.  Hanson, editor; DOE Document No.  DOE/TIC-22800;\\n    Government Printing Office, Washington, D.C., April 1980.)\\n\\n\\n    IMPACT OF THE SPACE SHUTTLE ON THE OZONE LAYER\\n\\n    From time to time, claims are made that chemicals released from\\n    the Space Shuttle\\'s Solid Rocket Boosters (SRBs) are responsible\\n    for a significant amount of damage to the ozone layer. Studies\\n    indicate that they in reality have only a minute impact, both in\\n    absolute terms and relative to other chemical sources. The\\n    remainder of this item is a response from the author of the quoted\\n    study, Charles Jackman.\\n\\n    The atmospheric modelling study of the space shuttle effects on the\\n    stratosphere involved three independent theoretical groups, and was\\n    organized by Dr. Michael Prather, NASA/Goddard Institute for Space\\n    Studies.  The three groups involved Michael Prather and Maria Garcia\\n    (NASA/GISS), Charlie Jackman and Anne Douglass (NASA/Goddard Space\\n    Flight Center), and Malcolm Ko and Dak Sze (Atmospheric and\\n    Environmental Research, Inc.).  The effort was to look at the effects\\n    of the space shuttle and Titan rockets on the stratosphere.\\n\\n    The following are the estimated sources of stratospheric chlorine:\\n\\n       Industrial sources:    300,000,000 kilograms/year\\n\\t  Natural sources:     75,000,000 kilograms/year\\n\\t  Shuttle sources:\\t  725,000 kilograms/year\\n\\n    The shuttle source assumes 9 space shuttles and 6 Titan rockets are\\n    launched yearly. Thus the launches would add less than 0.25% to the\\n    total stratospheric chlorine sources.\\n\\n    The effect on ozone is minimal:  global yearly average total ozone would\\n    be decreased by 0.0065%. This is much less than total ozone variability\\n    associated with volcanic activity and solar flares.\\n\\n    The influence of human-made chlorine products on ozone is computed\\n    by atmospheric model calculations to be a 1% decrease in globally\\n    averaged ozone between 1980 and 1990. The influence of the space shuttle and\\n    Titan rockets on the stratosphere is negligible.  The launch\\n    schedule of the Space Shuttle and Titan rockets would need to be\\n    increased by over a factor of a hundred in order to have about\\n    the same effect on ozone as our increases in industrial halocarbons\\n    do at the present time.\\n\\n    Theoretical results of this study have been published in _The Space\\n    Shuttle\\'s Impact on the Stratosphere_, MJ Prather, MM Garcia, AR\\n    Douglass, CH Jackman, M.K.W. Ko and N.D. Sze, Journal of Geophysical\\n    Research, 95, 18583-18590, 1990.\\n\\n    Charles Jackman, Atmospheric Chemistry and Dynamics Branch,\\n    Code 916, NASA/Goddard Space Flight Center,\\n    Greenbelt, MD  20771\\n\\n    Also see _Chemical Rockets and the Environment_, A McDonald, R Bennett,\\n    J Hinshaw, and M Barnes, Aerospace America, May 1991.\\n\\n\\n    HOW LONG CAN A HUMAN LIVE UNPROTECTED IN SPACE\\n\\n    If you *don\\'t* try to hold your breath, exposure to space for half a\\n    minute or so is unlikely to produce permanent injury. Holding your\\n    breath is likely to damage your lungs, something scuba divers have to\\n    watch out for when ascending, and you\\'ll have eardrum trouble if your\\n    Eustachian tubes are badly plugged up, but theory predicts -- and animal\\n    experiments confirm -- that otherwise, exposure to vacuum causes no\\n    immediate injury. You do not explode. Your blood does not boil. You do\\n    not freeze. You do not instantly lose consciousness.\\n\\n    Various minor problems (sunburn, possibly \"the bends\", certainly some\\n    [mild, reversible, painless] swelling of skin and underlying tissue)\\n    start after ten seconds or so. At some point you lose consciousness from\\n    lack of oxygen. Injuries accumulate. After perhaps one or two minutes,\\n    you\\'re dying. The limits are not really known.\\n\\n    References:\\n\\n    _The Effect on the Chimpanzee of Rapid Decompression to a Near Vacuum_,\\n    Alfred G. Koestler ed., NASA CR-329 (Nov 1965).\\n\\n    _Experimental Animal Decompression to a Near Vacuum Environment_, R.W.\\n    Bancroft, J.E. Dunn, eds, Report SAM-TR-65-48 (June 1965), USAF School\\n    of Aerospace Medicine, Brooks AFB, Texas.\\n\\n\\n    HOW THE CHALLENGER ASTRONAUTS DIED\\n\\n    The Challenger shuttle launch was not destroyed in an explosion. This is\\n    a well-documented fact; see the Rogers Commission report, for example.\\n    What looked like an explosion was fuel burning after the external tank\\n    came apart. The forces on the crew cabin were not sufficient to kill the\\n    astronauts, never mind destroy their bodies, according to the Kerwin\\n    team\\'s medical/forensic report.\\n\\n    The astronauts were killed when the more-or-less intact cabin hit the\\n    water at circa 200MPH, and their bodies then spent several weeks\\n    underwater. Their remains were recovered, and after the Kerwin team\\n    examined them, they were sent off to be buried.\\n\\n\\n    USING THE SHUTTLE BEYOND LOW EARTH ORBIT\\n\\n    You can\\'t use the shuttle orbiter for missions beyond low Earth orbit\\n    because it can\\'t get there. It is big and heavy and does not carry\\n    enough fuel, even if you fill part of the cargo bay with tanks.\\n\\n    Furthermore, it is not particularly sensible to do so, because much of\\n    that weight is things like wings, which are totally useless except in\\n    the immediate vicinity of the Earth. The shuttle orbiter is highly\\n    specialized for travel between Earth\\'s surface and low orbit. Taking it\\n    higher is enormously costly and wasteful. A much better approach would\\n    be to use shuttle subsystems to build a specialized high-orbit\\n    spacecraft.\\n\\n    [Yet another concise answer by Henry Spencer.]\\n\\n\\n    THE \"FACE ON MARS\"\\n\\n    There really is a big rock on Mars that looks remarkably like a humanoid\\n    face. It appears in two different frames of Viking Orbiter imagery:\\n    35A72 (much more facelike in appearance, and the one more often\\n    published, with the Sun 10 degrees above western horizon) and 70A13\\n    (with the Sun 27 degrees from the west).\\n\\n    Science writer Richard Hoagland has championed the idea that the Face is\\n    artificial, intended to resemble a human, and erected by an\\n    extraterrestrial civilization. Most other analysts concede that the\\n    resemblance is most likely accidental. Other Viking images show a\\n    smiley-faced crater and a lava flow resembling Kermit the Frog elsewhere\\n    on Mars. There exists a Mars Anomalies Research Society (sorry, don\\'t\\n    know the address) to study the Face.\\n\\n    The Mars Observer mission will carry an extremely high-resolution\\n    camera, and better images of the formation will hopefully settle this\\n    question in a few years. In the meantime, speculation about the Face is\\n    best carried on in the altnet group alt.alien.visitors, not sci.space or\\n    sci.astro.\\n\\n    V. DiPeitro and G. Molenaar, *Unusual Martian Surface Features*, Mars\\n    Research, P.O. Box 284, Glen Dale, Maryland, USA, 1982. $18 by mail.\\n\\n    R.R. Pozos, *The Face of Mars*, Chicago Review Press, 1986. [Account of\\n    an interdisciplinary speculative conference Hoagland organized to\\n    investigate the Face]\\n\\n    R.C. Hoagland, *The Monuments of Mars: A City on the Edge of Forever*,\\n    North Atlantic Books, Berkeley, California, USA, 1987. [Elaborate\\n    discussion of evidence and speculation that formations near the Face\\n    form a city]\\n\\n    M.J. Carlotto, \"Digital Imagery Analysis of Unusual Martian Surface\\n    Features,\" *Applied Optics*, 27, pp. 1926-1933, 1987. [Extracts\\n    three-dimensional model for the Face from the 2-D images]\\n\\n    M.J. Carlotto & M.C. Stein, \"A Method of Searching for Artificial\\n    Objects on Planetary Surfaces,\" *Journal of the British Interplanetary\\n    Society*, Vol. 43 no. 5 (May 1990), p.209-216. [Uses a fractal image\\n    analysis model to guess whether the Face is artificial]\\n\\n    B. O\\'Leary, \"Analysis of Images of the `Face\\' on Mars and Possible\\n    Intelligent Origin,\" *JBIS*, Vol. 43 no. 5 (May 1990), p. 203-208.\\n    [Lights Carlotto\\'s model from the two angles and shows it\\'s consistent;\\n    shows that the Face doesn\\'t look facelike if observed from the surface]\\n\\n\\nNEXT: FAQ #13/15 - Space activist/interest/research groups & space publications\\n'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.first()['document'][0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8jIEJFf9kvh"
   },
   "source": [
    "### Annotators\n",
    "\n",
    "Now we look at stage 3—the annotators. This is the heart of the NLP work. So let's look at some of the Annotators available in Spark NLP.\n",
    "\n",
    "We will look at some commonly used annotators:\n",
    "\n",
    "* `SentenceDetector`\n",
    "* `Tokenizer`\n",
    "* `Lemmatizer`\n",
    "* `PerceptronApproach` (POSTagger)\n",
    "\n",
    "#### SentenceDetector\n",
    "\n",
    "The SentenceDetector uses a rule-based algorithm inspired by Kevin Dias's Ruby implementation. It takes the following parameters\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to sentence-tokenize.\n",
    "* outputCol  \n",
    "The name of the new sentence column.\n",
    "* useAbbrevations  \n",
    "Determines whether to apply abbreviations at sentence detection.\n",
    "* useCustomBoundsOnly  \n",
    "Determines whether to only utilize custom bounds for sentence detection.\n",
    "* explodeSentences  \n",
    "Determines whether to explode each sentence into a different row, for better parallelization. Defaults to false.\n",
    "* customBounds  \n",
    "Characters used to explicitly mark sentence bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWKNV7bV9ixl"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import SentenceDetector\n",
    "\n",
    "sent_detector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "SdJ5Nq-c97xF",
    "outputId": "b4d27239-f63d-4d80-b28c-1294a2dab17d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "      <td>[(document, 0, 963, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 924, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 676, Path: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 64, Path: cantaloupe.srv.cs.cmu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 176, Path: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....   \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...   \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...   \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...   \n",
       "\n",
       "                                           sentences  \n",
       "0  [(document, 0, 963, Xref: cantaloupe.srv.cs.cm...  \n",
       "1  [(document, 0, 924, Xref: cantaloupe.srv.cs.cm...  \n",
       "2  [(document, 0, 676, Path: cantaloupe.srv.cs.cm...  \n",
       "3  [(document, 0, 64, Path: cantaloupe.srv.cs.cmu...  \n",
       "4  [(document, 0, 176, Path: cantaloupe.srv.cs.cm...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_detector.transform(docs)\n",
    "sentences.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29gzaJ2Z995D"
   },
   "source": [
    "#### Tokenizer\n",
    "\n",
    "A Tokenizer is a fundamental Annotator. Almost all text-based data processing begins with some form of tokenization. Most classical NLP algorithms expect tokens as the basic input. Many deep learning algorithms are being developed that take characters as basic input. Most NLP applications still use tokenization. The Spark NLP Tokenizer is a little more sophisticated than just a regular expression-based tokenizer. It has a number of parameters. The following are some of the basic ones (see table0316 for the results):\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tokenize.\n",
    "* outputCol  \n",
    "The name of the new token column.\n",
    "* targetPattern  \n",
    "Basic regex rule to identify a candidate for tokenization. Defaults to \\S+ which means anything not a space (optional).\n",
    "* prefixPattern  \n",
    "Regular expression (regex) to identify subtokens that come in the beginning of the token. Regex has to start with \\A and must contain groups (). Each group will become a separate token within the prefix. Defaults to nonletter characters—for example, quotes or parentheses (optional).\n",
    "* suffixPattern  \n",
    "Regex to identify subtokens that are in the end of the token. Regex has to end with \\z and must contain groups (). Each group will become a separate token within the prefix. Defaults to nonletter characters—for example, quotes or parentheses (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVupoF4n98wt"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences'])\\\n",
    "    .setOutputCol('tokens')\\\n",
    "    .fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "gU3iypI_-JwE",
    "outputId": "078fed66-fb77-449e-f0f9-176e9784d36f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "      <td>[(document, 0, 963, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 924, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 676, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 64, Path: cantaloupe.srv.cs.cmu...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 176, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....   \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...   \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...   \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 963, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 924, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 676, Path: cantaloupe.srv.cs.cm...   \n",
       "3  [(document, 0, 64, Path: cantaloupe.srv.cs.cmu...   \n",
       "4  [(document, 0, 176, Path: cantaloupe.srv.cs.cm...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.transform(sentences)\n",
    "tokens.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zbowbqm6-Lwk"
   },
   "source": [
    "There are some `Annotators` that require additional resources. Some require reference data, like the following example, the lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khm7EB4C-QM-"
   },
   "source": [
    "#### Lemmatizer\n",
    "\n",
    "The lemmatizer finds the lemmas for the tokens. Lemmas are the entry words in dictionaries. For example, \"cats\" lemmatizes to \"cat,\" and \"oxen\" lemmatizes to \"ox.\" Loading the lemmatizer requires a dictionary and the following three parameters:\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tokenize\n",
    "* outputCol  \n",
    "The name of the new token column\n",
    "* dictionary  \n",
    "The resource to be loaded as the lemma dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0eNjBa0-Kxk"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Lemmatizer\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "  .setInputCols([\"tokens\"]) \\\n",
    "  .setOutputCol(\"lemma\") \\\n",
    "  .setDictionary('../en_lemmas.txt', '\\t', ',')\\\n",
    "  .fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "jH-klgbV-Wpz",
    "outputId": "c56166aa-846f-4c47-b204-5b8b451d92f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "      <td>[(document, 0, 963, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 924, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 676, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 64, Path: cantaloupe.srv.cs.cmu...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 176, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....   \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...   \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...   \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 963, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 924, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 676, Path: cantaloupe.srv.cs.cm...   \n",
       "3  [(document, 0, 64, Path: cantaloupe.srv.cs.cmu...   \n",
       "4  [(document, 0, 176, Path: cantaloupe.srv.cs.cm...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "\n",
       "                                               lemma  \n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = lemmatizer.transform(tokens)\n",
    "lemmas.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tfE16_Q_Bmy"
   },
   "source": [
    "#### POS tagger\n",
    "\n",
    "There are also Annotators that require models as resources. For example, the POS tagger uses a perceptron model, so it is called PerceptronApproach. The PerceptronApproach has five parameters:\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tag\n",
    "* outputCol  \n",
    "The name of the new tag column\n",
    "* posCol  \n",
    "Column of Array of POS tags that match tokens\n",
    "* corpus  \n",
    "POS tags delimited corpus; needs \"delimiter\" in options\n",
    "* nIterations  \n",
    "Number of iterations in training, converges to better accuracy\n",
    "\n",
    "We will load a pretrained model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5Y3jCTf-_0n"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import PerceptronModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "JUd68mQ1_N_y",
    "outputId": "be1abe00-3b09-4467-ab6a-11c6534e265d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = PerceptronModel.pretrained() \\\n",
    "  .setInputCols([\"tokens\", \"sentences\"]) \\\n",
    "  .setOutputCol(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "lCFlMYyD_PBL",
    "outputId": "a4410bba-a387-490d-bf20-5979c38e0142"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "      <td>[(document, 0, 963, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Xref'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 924, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Xref'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 676, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 64, Path: cantaloupe.srv.cs.cmu...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 176, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....   \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...   \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...   \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 963, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 924, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 676, Path: cantaloupe.srv.cs.cm...   \n",
       "3  [(document, 0, 64, Path: cantaloupe.srv.cs.cmu...   \n",
       "4  [(document, 0, 176, Path: cantaloupe.srv.cs.cm...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [(pos, 0, 3, NNP, {'word': 'Xref'}, []), (pos,...  \n",
       "1  [(pos, 0, 3, NNP, {'word': 'Xref'}, []), (pos,...  \n",
       "2  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  \n",
       "3  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  \n",
       "4  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags = pos_tagger.transform(lemmas)\n",
    "postags.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAAqz_pO_TLX"
   },
   "source": [
    "### Pretrained Pipelines\n",
    "\n",
    "We saw earlier how we can organize multiple MLlib stages into a Pipeline. Using Pipelines is especially useful in NLP tasks because there are often many stages between loading the raw text and extracting structured data.\n",
    "\n",
    "Spark NLP has pretrained pipelines that can be used to process text. This doesn't mean that you do not need to tune pipelines for application. But it is often convenient to begin experimenting with a prebuilt NLP pipeline and find what needs tuning.\n",
    "\n",
    "#### Explain document ML pipeline\n",
    "\n",
    "The BasicPipeline does sentence splitting, tokenization, lemmatization, stemming, and POS tagging. If you want to get a quick look at some text data, this is a great pipeline to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "H8Yt5ByW_QVr",
    "outputId": "c83932e5-f4e8-4bf8-bf78-1b0310bf51eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pipeline = PretrainedPipeline('explain_document_ml', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "aKC7kAht_dib",
    "outputId": "0e7c2ee3-1a84-4d6c-854a-ccabdc1b360e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>spell</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[(document, 0, 13095, Xref: cantaloupe.srv.cs....</td>\n",
       "      <td>[(document, 0, 963, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NN, {'word': 'xref'}, []), (pos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[(document, 0, 8806, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 924, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, tref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, xref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NN, {'word': 'tref'}, []), (pos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[(document, 0, 1977, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 676, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1453, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 64, Path: cantaloupe.srv.cs.cmu...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[(document, 0, 1191, Path: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 176, Path: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, Path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, path, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 13095, Xref: cantaloupe.srv.cs....   \n",
       "1  [(document, 0, 8806, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1977, Path: cantaloupe.srv.cs.c...   \n",
       "3  [(document, 0, 1453, Path: cantaloupe.srv.cs.c...   \n",
       "4  [(document, 0, 1191, Path: cantaloupe.srv.cs.c...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [(document, 0, 963, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 924, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 676, Path: cantaloupe.srv.cs.cm...   \n",
       "3  [(document, 0, 64, Path: cantaloupe.srv.cs.cmu...   \n",
       "4  [(document, 0, 176, Path: cantaloupe.srv.cs.cm...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0'}, []), (...   \n",
       "\n",
       "                                               spell  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, tref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "3  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "4  [(token, 0, 3, Path, {'sentence': '0', 'confid...   \n",
       "\n",
       "                                               stems  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, xref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 3, path, {'sentence': '0', 'confid...   \n",
       "3  [(token, 0, 3, path, {'sentence': '0', 'confid...   \n",
       "4  [(token, 0, 3, path, {'sentence': '0', 'confid...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [(pos, 0, 3, NN, {'word': 'xref'}, []), (pos, ...  \n",
       "1  [(pos, 0, 3, NN, {'word': 'tref'}, []), (pos, ...  \n",
       "2  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  \n",
       "3  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  \n",
       "4  [(pos, 0, 3, NNP, {'word': 'Path'}, []), (pos,...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o82tgVLv_ees"
   },
   "outputs": [],
   "source": [
    "text = texts.first()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "fCvfeIzy_g8D",
    "outputId": "6df32850-cf6a-4476-eb80-675926364268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('much', 'much', 'much'),\n",
       " ('argument', 'argum', 'argument'),\n",
       " ('and', 'and', 'and'),\n",
       " ('few', 'few', 'few'),\n",
       " ('facts', 'fact', 'fact'),\n",
       " ('being', 'be', 'be'),\n",
       " ('offered', 'offer', 'offer'),\n",
       " ('.', '.', '.'),\n",
       " ('The', 'the', 'The'),\n",
       " ('summaries', 'summari', 'summary'),\n",
       " ('below', 'below', 'below'),\n",
       " ('attempt', 'attempt', 'attempt'),\n",
       " ('to', 'to', 'to'),\n",
       " ('represent', 'repres', 'represent'),\n",
       " ('the', 'the', 'the'),\n",
       " ('position', 'posit', 'position'),\n",
       " ('on', 'on', 'on'),\n",
       " ('which', 'which', 'which'),\n",
       " ('much', 'much', 'much'),\n",
       " ('of', 'of', 'of')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = pipeline.annotate(text)\n",
    "list(zip(\n",
    "    annotations['token'], \n",
    "    annotations['stems'], \n",
    "    annotations['lemmas']\n",
    "))[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNJkHpxA_jp8"
   },
   "source": [
    "There are many other pipelines, and there is additional information [available](https://nlp.johnsnowlabs.com/docs/en/pipelines).\n",
    "\n",
    "Now let's talk about how we will perform step 4, converting the annotations into native Spark SQL types using the Finisher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIm6p2iw_qM_"
   },
   "source": [
    "### Finisher\n",
    "\n",
    "The annotations are useful for composing NLP steps, but we generally want to take some specific information out to process. The `Finisher` handles most of these use cases. If you want to get a list of tokens (or stems, or what have you) to use in downstream MLlib stages, the `Finisher` can do this (see table0320). Let's look at the parameters:\n",
    "\n",
    "* inputCols  \n",
    "Name of input annotation cols\n",
    "* outputCols  \n",
    "Name of finisher output cols\n",
    "* valueSplitSymbol  \n",
    "Character separating annotations\n",
    "* annotationSplitSymbol  \n",
    "Character separating annotations\n",
    "* cleanAnnotations  \n",
    "Determines whether to remove annotation columns\n",
    "* includeMetadata  \n",
    "Annotation metadata format\n",
    "* outputAsArray  \n",
    "Finisher generates an Array with the results instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0OGNNL4_iKC"
   },
   "outputs": [],
   "source": [
    "finisher = Finisher()\\\n",
    "    .setInputCols(['tokens', 'lemma'])\\\n",
    "    .setOutputCols(['tokens', 'lemmata'])\\\n",
    "    .setCleanAnnotations(True)\\\n",
    "    .setOutputAsArray(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tJTzbG5_2na"
   },
   "outputs": [],
   "source": [
    "custom_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sent_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    finisher\n",
    "]).fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "jq23jmJi_3mq",
    "outputId": "491ff008-9fb0-40f8-a8e4-df463f5b6dc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "2  [Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....   \n",
       "3  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "4  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "\n",
       "                                             lemmata  \n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...  \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...  \n",
       "2  [Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....  \n",
       "3  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...  \n",
       "4  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp02kNkt_40K"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRnUE19R_6Nq"
   },
   "outputs": [],
   "source": [
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8l61JlZ_7ZC"
   },
   "outputs": [],
   "source": [
    "larger_pipeline = Pipeline(stages=[\n",
    "    custom_pipeline,\n",
    "    StopWordsRemover(\n",
    "        inputCol='lemmata', \n",
    "        outputCol='terms', \n",
    "        stopWords=stopwords)\n",
    "]).fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "PhPy7oyi_80K",
    "outputId": "0757436a-4324-4c81-c1ae-af7f1125915f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/alex/projects/spark-nlp-book-prod/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "      <td>[Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "1  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "2  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "3  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "4  file:/home/alex/projects/spark-nlp-book-prod/j...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5984...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.space:5990...   \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....   \n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "2  [Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....   \n",
       "3  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "4  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...   \n",
       "2  [Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....   \n",
       "3  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "4  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...   \n",
       "\n",
       "                                               terms  \n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...  \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.space...  \n",
       "2  [Path, :, cantaloupe.srv.cs.cmu.edu!crabapple....  \n",
       "3  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...  \n",
       "4  [Path, :, cantaloupe.srv.cs.cmu.edu!das-news.h...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fste5GTS__PC"
   },
   "source": [
    "Now that we have reviewed Spark and Spark NLP, we are almost ready to start building an NLP application. There is an extra benefit to learning an annotation library—it helps you understand how to structure a pipeline for NLP. This knowledge will be applicable even if you are using other technologies.\n",
    "\n",
    "The only topic left for us to cover is deep learning, which we cover in the next chapter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vteG8sRHABoj"
   },
   "source": [
    "## Exercises: Build a Topic Model\n",
    "   One of the easiest things you can do to begin exploring your data set is to create a topic model. To do this we need to transform the text into numerical vectors. We will go into this more in the next part of the book. For now, let's build a pipeline for processing text.\n",
    "\n",
    "First, we need to split the texts into sentences. Second, we need to tokenize. Next, we need to normalize our words with a lemmatizer and the normalizer. After this, we need to finish our pipeline and remove stop words. (So far, everything except the Normalizer has been demonstrated in this chapter.) After that, we will pass the information into our topic modeling pipeline.\n",
    "\n",
    "Check the [online documentation](https://nlp.johnsnowlabs.com/docs/en/annotators#normalizer) for help with the normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWJCZYJi_-Ky"
   },
   "outputs": [],
   "source": [
    "# document_assembler = ???\n",
    "# sent_detector = ???\n",
    "# tokenizer = ???\n",
    "# lemmatizer = ???\n",
    "# normalizer = ???\n",
    "# finisher = ???\n",
    "# sparknlp_pipeline = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TGvmB2wANDa"
   },
   "outputs": [],
   "source": [
    "# stopwords = ???\n",
    "# stopword_remover = ??? # use outputCol='terms'\n",
    "\n",
    "#text_processing_pipeline = ??? # first stage is sparknlp_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dp3hWHBXAOwK"
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import CountVectorizer, IDF\n",
    "# from pyspark.ml.clustering import LDA\n",
    "\n",
    "# tf = CountVectorizer(inputCol='terms', outputCol='tf')\n",
    "# idf = IDF(inputCol='tf', outputCol='tfidf')\n",
    "# lda = LDA(k=10, seed=123, featuresCol='tfidf')\n",
    "\n",
    "# pipeline = Pipeline(stages=[\n",
    "#     text_processing_pipeline,\n",
    "#     tf,\n",
    "#     idf,\n",
    "#     lda\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_X7MIcdfAQlr"
   },
   "outputs": [],
   "source": [
    "# model = pipeline.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNAMioU2ARmL"
   },
   "outputs": [],
   "source": [
    "# tf_model = model.stages[-3]\n",
    "# lda_model = model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoyIS8AQASx6"
   },
   "outputs": [],
   "source": [
    "# topics = lda_model.describeTopics().collect()\n",
    "# for k, topic in enumerate(topics):\n",
    "#     print('Topic', k)\n",
    "#     for ix, wt in zip(topic['termIndices'], topic['termWeights']):\n",
    "#         print(ix, tf_model.vocabulary[ix], wt)\n",
    "#     print('#' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7p1GEHUjAUmT"
   },
   "source": [
    "Congratulations! You have built your first full Spark pipeline with Spark NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "639galEhAXTn"
   },
   "source": [
    "Resources\n",
    "\n",
    "* Spark API: this is a great resource to have handy. With each new release there can be new additions, so I rely on the API documentation to keep up to date.\n",
    "  * Scala\n",
    "  * Java\n",
    "  * Python\n",
    "  * R\n",
    "  * Spark SQL programming guide\n",
    "  * MLlib programming guide\n",
    "* Natural Language Processing with Python: this is the book written explaining NLP with NLTK. It is available for free online.\n",
    "* spaCy: this library is doing a lot of cool things and has a strong community.\n",
    "* Spark NLP: the website of the Spark NLP library. Here you can find documentation, tutorials, and videos about the library.\n",
    "* Foundations of Statistical Natural Language Processing by Christopher D. Manning and Hinrich Schütze (MIT Press)\n",
    "  * This is considered a classic for natural language processing. It is an older book, so you won't find information on deep learning. If you want to have a strong foundation in NLP this is a great book to have.\n",
    "* Spark: The Definitive Guide by Matei Zaharia and Bill Chambers (O'Reilly)\n",
    "  * This is a great resource to have when using Spark.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyFaFiU0BTzUWEmuo57HPB",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1.3_NLP_on_Apache_Spark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
